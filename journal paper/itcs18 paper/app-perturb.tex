We show that Assumption~\eqref{eq:assn-distinct} holds almost surely under a small random perturbation of the prior. We focus on problem instances with 0-1 rewards, and assume that the prior $\priorMu$ is independent across arms and has a finite support.%
\footnote{The assumption of 0-1 rewards is for clarity. Our results hold under a more general assumption that for each arm $a$, rewards can only take finitely many values, and each of these values is possible (with positive probability) for every possible value of the mean reward $\mu_a$.}
 Consider the probability vector in the prior for arm $a$:
\[ \vec{p}_a = \left(\; \Pr[\mu_a=\nu]:\; \nu\in \support(\mu_a)\; \right).\]
We apply a small random perturbation independently to each such vector:
\begin{align}\label{eq:app:perturb:noise}
\vec{p}_a \leftarrow \vec{p}_a + \vec{q}_a,
    \quad \text{where}\quad \vec{q}_a\sim  \mN_a.
\end{align}
Here $\mN_a$ is the noise distribution for arm $a$: a distribution over real-valued, zero-sum vectors of dimension $d_a = |\support(\mu_a)|$. We need the noise distribution to satisfy the following property:
\begin{align}\label{eq:app:perturb:noise-prop}
\forall x\in [-1,1]^{d_a}\setminus \{0\}\qquad
\Pr_{q\sim \mN_a}\left[ x\cdot(\vec{p}_a+ q) \neq 0 \right] =1.
\end{align}

\begin{theorem}\label{thm:perturb}
Consider an instance of MAB with 0-1 rewards. Assume that the prior $\priorMu$ is independent across arms, and each mean reward $\mu_a$ has a finite support that does not include $0$ or $1$. Assume that noise distributions $\mN_a$ satisfy property \eqref{eq:app:perturb:noise-prop}. If random perturbation~\eqref{eq:app:perturb:noise} is applied independently to each arm $a$, then \refeq{eq:assn-distinct} holds almost surely for each history $h$.
\end{theorem}

\begin{remark}
As a generic example of a noise distribution which satisfies
  Property \eqref{eq:app:perturb:noise-prop}, consider the uniform
  distribution $\mN$ over the bounded convex set
  \[ Q = \left\{q \in \R^{d_a} \mid q \cdot \vec{1} = 0 \mbox{ and }
      \|q\|_2 \leq \eps\right\}, \] where $\vec{1}$ denotes the all-1
  vector. If $x = a \vec{1}$ for some non-zero value of $a$, then
  \eqref{eq:app:perturb:noise-prop} holds because
  $$x \cdot (p+q) = x \cdot p = a\neq 0.$$ Otherwise, denote
  $p=\vec{p}_a$ and observe that $x\cdot({p}+ {q}) = 0$ only if
  $x \cdot q = c \triangleq x \cdot (-p)$. Since $x\neq \vec{1}$, the
  intersection $Q\cap\{ x\cdot q = c \}$ either is empty or has
  measure 0 in $Q$, which implies
  $\Pr_{{q}}\left[ x\cdot({p}+ {q}) \neq 0 \right] =1$.
\end{remark}


To prove Theorem~\ref{thm:perturb}, it suffices to focus on two arms, and perturb one of them. Since realized rewards have finite support, there are only finitely many possible histories. Therefore, it suffices to focus on a fixed history $h$.

\begin{lemma}\label{lm:perturb}
Consider an instance of MAB with 0-1 rewards. Assume that the prior $\priorMu$ is independent across arms, and that $\support(\mu_1)$ is finite and does not include $0$ or $1$. Fix history $h$. Suppose random perturbation~\eqref{eq:app:perturb:noise} is applied to arm $1$, with noise distribution $\mN_1$ that satisfies \eqref{eq:app:perturb:noise-prop}. Then
    $\E[\mu_1\mid h] \neq \E[\mu_2\mid h] $
almost surely.
\end{lemma}

\begin{proof}
Note that $\E[\mu_a\mid h]$ does not depend on the algorithm which produced this history. Therefore, for the sake of the analysis, we can assume w.l.o.g. that this history has been generated by a particular algorithm, as long as this algorithm can can produce this history with non-zero probability. Let us consider  the algorithm that deterministically chooses same actions as $h$.

Let $S = \support(\mu_1)$. Then:
\begin{align*}
\E[\mu_1\mid h]
    &= \sum_{\nu\in S} \nu \cdot \Pr[\mu_1 =\nu \mid h]
    = \sum_{\nu\in S} \nu \cdot \Pr[h \mid \mu_1 =\nu] \cdot \Pr[\mu_1=\nu]\;/\;\Pr[h], \\
\Pr[h]
    &= \sum_{\nu\in S} \Pr[h \mid \mu_1 =\nu] \cdot \Pr[\mu_1=\nu].
\end{align*}
Therefore,
    $\E[\mu_1\mid h] = \E[\mu_2\mid h] $
if and only if
\begin{align*}
\sum_{\nu\in S} (\nu-C) \cdot \Pr[h \mid \mu_1 =\nu] \cdot \Pr[\mu_1=\nu] = 0,
\quad\text{ where }\quad C=\E[\mu_2\mid h].
\end{align*}
Since $\E[\mu_2\mid h]$ and $\Pr[h \mid \mu_1 =\nu]$ do not depend on the probability vector $\vec{p}_1$, we conclude that
\begin{align*}
  \E[\mu_1\mid h] = \E[\mu_2\mid h]
\quad\Leftrightarrow\quad x\cdot \vec{p}_1 =0,
\end{align*}
where vector
    \[ x := \left(\; (\nu-C) \cdot \Pr[h \mid \mu_1 =\nu]:\; \nu\in S\; \right) \in [-1,1]^{d_1}\]
does not depend on $\vec{p}_1$.

Thus, it suffices to prove that $x\cdot \vec{p}_1 \neq 0$ almost surely under the perturbation. In a formula:
\begin{align}\label{eq:lm:perturb-pf}
    \Pr_{q\sim \mN_1}\left[ x\cdot( \vec{p}_1+q) \neq 0 \right] =1
\end{align}

Note that $\Pr[h \mid \mu_1 =\nu]>0$ for all $\nu\in S$, because $0,1\not\in S$. It follows that at most one coordinate of $x$ can be zero. So  \eqref{eq:lm:perturb-pf} follows from property \eqref{eq:app:perturb:noise-prop}.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main.tex"
%%% End:
