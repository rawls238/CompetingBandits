\documentclass[../competing_bandits.tex]{subfiles}
\begin{document}

OBSOLETE!

%\subsubsection{Simulation details.}
%\label{section:3}

\subsubsection{MAB instances.}
In all experiments, there are $K=10$ arms. Since we focus on 0-1 rewards, an instance of the MAB problem is specified by the mean reward vector $(\mu(a):\; a\in A)$, henceforth \MRV. Initially the \MRV is drawn from some distribution, termed \emph{MAB instance}. We consider several representative MAB instances.
\begin{enumerate}
\item \emph{Needle-In-Haystack}: one arm (the ``needle") is chosen uniformly at random. This arm has mean reward $.7$, and the remaining ones have mean reward $.5$.

\item \emph{Uniform instance}: the mean reward of each arm is drawn independently and uniformly from $[0.25, 0.75]$.
\item \emph{Heavy-tail instance}: the mean reward of each arm is drawn independently from $\Beta(.6,.6)$ distribution.%
    \footnote{This distribution has substantial ``tail probabilities".}
\end{enumerate}

\subsubsection{Terminology.}
\asedit{Following a standard game-theoretic terminology, algorithm Alg1 \emph{(weakly) dominates} algorithm Alg2 for a given firm if Alg1 provides a larger (or equal) market share than Alg2 at the end of the game. An algorithm is a (weakly) dominant strategy for the firm if it (weakly) dominates all other algorithms. This is for a particular MAB instance and a particular selection of the game parameters.}

\OMIT{\gaedit{Since the algorithms that involve exploration have larger deployment costs, we break any indifference towards algorithms that do not involve exploration (i.e. if \DynamicGreedy and \DynamicEpsGreedy provide the same market share, the firm will play \DynamicGreedy).}}


\subsubsection{Simulation details.}
For each MAB instance we draw $N = 1000$ \MRVs independently from the corresponding distribution. We use this same collection of \MRVs for all experiments with this MAB instance. For each \MRV we draw a table of realized rewards (\emph{realization table}), and use this same table for all experiments on this \MRV. This ensures that differences in algorithm performance are not due to noise in the realizations but due to differences in the algorithms in the different experimental settings.

More specifically, the realization table is a 0-1 matrix $W$ with $K$ columns which correspond to arms, and $T+T_{\max}$ rows, which correspond to rounds. Here $T_{\max}$ is the maximal duration of the ``warm start" in our experiments, \ie the maximal value of $X+T_0$. For each arm $a$, each value $W(\cdot,a)$ is drawn independently from Bernoulli distribution with expectation $\mu(a)$. Then in each experiment, the reward of this arm in round $t$ of the warm start is taken to be $W(t,a)$, and its reward in round $t$ of the game is $W(T_{\max}+t,a)$.

We fix the sliding window size $M = 100$. We found that lower values induced too much random noise in the results, and increasing $M$ further did not make a qualitative difference. Unless otherwise noted, \asedit{we used $T = 2000$}. 

\asedit{The simulations are computationally intensive. An experiment on a particular MAB instance comprised multiple runs of the competition game: $N$ mean reward vectors times $9$ pairs of algorithms times three values for the warm start. We used a parallel implementation over a cluster of 12 XX Hz CPU cores, with 8 GB RAM per core. With this implementation, each experiment took about XX hours.}

While we experiments with various MAB instances and parameter settings, in the main paper we only report plots for selected, representative experiments. Additional plots can be found in the supplement. Unless noted otherwise, our findings are based on and consistent with all these plots.

\end{document} 