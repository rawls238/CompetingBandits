\documentclass[../competing_bandits_with_appendix.tex]{subfiles}
\begin{document}

\section{Introduction}\label{sec:intro}

Many modern online platforms simultaneously compete for users as well as learn from the users they manage to attract. This creates a tension between \textit{exploration} and \textit{competition}: firms experiment with potentially sub-optimal options for the sake of gaining information to make better decisions tomorrow, while they need to incentivize consumers to select them over their competitors today. For instance, Google Search and Bing compete for users in the search engine market yet at the same time need to experiment with their search and ranking algorithms to learn what works best. Similar exploration vs. competition tension arises in other application domains: recommendation systems, news and entertainment websites, online commerce, and so forth.

Online platforms routinely deploy A/B tests, and are increasingly adopting  more sophisticated exploration methodologies based on \emph{multi-armed bandits}, a well-known framework for making decisions under uncertainty and trading off exploration and exploitation (making good near-term decisions). While deploying ``better" learning algorithms for exploration would improve performance, this is not necessarily beneficial under competition, even putting aside the deployment/maintenance costs. In particular, excessive experimentation may hurt a platform's reputation and decrease its market share in the near term. This would leave the learning algorithm with less users to learn from, which may further degrade the platform's performance relative to competitors who keep learning and improving from \emph{their} users, and so forth. Taken to the extreme, such dynamics may create a ``death spiral" effect when the vast majority of customers eventually switch to competitors.

In this paper, we ask how the interplay of exploration and competition affects platforms' incentives. While some bandit algorithms are traditionally considered ``better" than others in the literature, {\bf\em does competition incentivize the adoption of the better algorithms}? How is this affected by the intensity of competition? We investigate these issues via extensive numerical experiments in a stylized duopoly model.

\xhdr{Our model.} We consider two firms that compete for users and simultaneously learn from them. Each firm commits to a multi-armed bandit algorithm, and \emph{explores} according to this algorithm. Users select between the two firms based on the current reputation score: rewards from the firm's algorithm, averaged over a recent time window. Each firm's objective is to maximize its market share (the fraction of users choosing this firm).



We consider a \emph{permanent duopoly} in which both firms start at the same time, as well as \emph{temporary monopoly}: a duopoly with a first-mover. Accordingly, the intensity of competition in the model varies from ``permanent monopoly" (just one firm) to ``incumbent" (the first-mover in temporary monopoly) to permanent duopoly to ``entrant" (late-arriver in temporary monopoly).%
\footnote{\asedit{We consider the ``permanent monopoly" scenario for comparison only, without presenting any findings. We just assume that a monopolist chooses
the greedy algorithm, because it is easier to deploy in practice. Implicitly, users have no ``outside option": the service provided is an improvement over not having it (and therefore the monopolist is not incentivized to deploy better learning algorithms). This is plausible with free ad-supported platforms such as Yelp or Google.}}

We focus on three classes of bandit algorithms, ranging from more primitive to more sophisticated: \emph{greedy algorithms} that do not explicitly explore, algorithms that separate exploration and exploitation, and algorithms that combine the two. We know from prior work that in the absence of competition,  ``better" algorithms are better in the long run, but could be worse initially.


\xhdr{Main findings.}
We find that in the permanent duopoly, competition incentivizes firms to choose the ``greedy algorithm", and even more so if the firm is a late arriver in a market. This algorithm also prevails under monopoly, simply because it tends to be easier to deploy. Whereas the incumbent in the temporary monopoly is incentivized to deploy a more advanced exploration algorithm. As a result, consumer welfare is highest under temporary monopoly. We find strong evidence of the ``death spiral" effect mentioned earlier; this effect is strongest under permanent duopoly.

Interpreting the adoption of better algorithms as ``innovation", our
findings can be framed in terms of the ``inverted-U relationship"
between competition and innovation (see Figure~\ref{fig:inverted-U}).
This is a well-established concept in the economics literature, dating
  back to \cite{Schumpeter-42}, whereby too
  little or too much competition is bad for innovation, but
  intermediate levels of competition tend to be better. However, the
``inverted-U relationship'' is driven by different aspects in our
model than the ones in the existing literature in economics. In our
case, the barriers for innovations arise entirely from the
reputational consequences of exploration in competition, as opposed to
the R\&D costs (which is the more standard cause in prior work).

\tikzstyle{level 1}=[level distance=3.5cm, sibling distance=4.0cm]
\tikzstyle{level 2}=[level distance=3.5cm, sibling distance=2cm]
\tikzstyle{below} = [align=center]

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}
      \draw[->] (-.5,0) -- (7,0) node[right] {};
      \draw[->] (0,-.5) -- (0,3) node[above] {Better algorithms};
      \draw[scale=0.6,domain=0.7:9.8,smooth,variable=\x,blue, line width=0.3mm] plot ({\x},{4.5 - 0.18 * (\x - 5.25)^2});
     \node[below] at (0.7, -0.22) {\footnotesize monopoly};
     \node[below] at (3, -0.2) {\footnotesize incumbent};
     \node[below] at (4.5, -0.22) {\footnotesize duopoly};
     \node[below] at (6, -0.2) {\footnotesize entrant};
 \end{tikzpicture}
 \caption{A stylized ``inverted-U relationship" between strength of competition and ``level of innovation".}
\label{fig:inverted-U}
\end{center}
\end{figure}

\xhdr{Additional findings.}
We investigate the ``first-mover advantage" phenomenon in more detail. Being first in the market gives free data to learn from (a ``data advantage") as well as a more definite, and possibly better reputation compared to an entrant (a ``reputation advantage"). We run additional experiments so as to isolate and compare these two effects. We find that either effect alone leads to a significant advantage under competition. The data advantage is larger than reputation advantage when the incumbent commits to a more advanced bandit algorithm.

Data advantage is significant from the anti-trust perspective, as a possible barrier to entry. We find that even a small amount ``data advantage" gets amplified under competition, causing a large difference in eventual market shares. This observation runs contrary to prior work  \cite{varian2018artificial,lambrecht2015can,bajari2018impact}, which studied learning without competition, and found that small amounts of additional data do not provide significant improvement in eventual outcomes. We conclude that competition dynamics -- that firms compete as they learn over time -- are pertinent to these anti-trust considerations.

We also investigate how algorithms' performance ``in isolation" (without competition) is predictive of the outcomes under competition. We find that mean reputation -- arguably, the most natural performance measure ``in isolation" -- is sometimes not a good predictor. We suggest a
more refined performance measure, and use it to explain some of the competition outcomes.


We also consider an alternative choice rule with explicit noise/randomness: a small fraction of users choose a firm uniformly at random.%
\footnote{\asedit{Reputation scores already introduce some noise into users' choices. However, the amount of noise due to this channel is typically small, both in our simulations and in practice, because reputation signals average over many datapoints.}} We confirm the theoretical intuition that better algorithms prevail if the expected number of ``random" users is sufficiently large. However, we find that this effect is negligible for some smaller but still ``relevant" parameter values.

\subsection{Discussion}

The present paper is an experimental counterpart to \cite{CompetingBandits-itcs18}, which considered a similar duopoly model and obtained a number of theoretical results with ``asymptotic" flavor. For the sake of analytical tractability, that paper makes a somewhat unrealistic simplification that users do not observe any signals about firms' ongoing performance. Instead, users choose between firms according to the firms' Bayesian-expected rewards. The strength of competition is varied in a different way, using assumptions about (ir)rational user behavior. \asedit{For these reasons, the theorems from \cite{CompetingBandits-itcs18} have no direct bearing on our simulations.} However, their high-level conclusion in is an inverted-U relationship similar to ours.


The present paper provides a more nuanced and ``non-asymptotic" perspective. In essence, we look for substantial effects within relevant time scales. Indeed, we start our investigation by determining what time scales are relevant in the context of our model. The reputation-based choice model accounts for competition in a more direct way, allows to separate reputation vs. data advantage, and makes our model amenable to numerical simulations (unlike the model in \cite{CompetingBandits-itcs18}).

To elucidate the interplay of competition and exploration, our model is stylized in several important  respects, some of which we discuss below. Firms compete only on the quality of service, rather than, say, pricing or the range of products. Various performance signals available to the users, from personal experience to word-of-mouth to consumer reports, are abstracted as persistent ``reputation scores", which further simplified to average performance over a time window.  On the machine learning side, our setup is geared to distinguish between ``simple" vs. ``better" vs. ``smart" bandit algorithms; we are not interested in state-of-art algorithms for very realistic bandit settings.

Even with a stylized model, numerical investigation is quite challenging. An ``atomic experiment" is a competition game between a given pair of bandit algorithms, in a given competition model, on a given instance of a multi-armed bandit problem.%
\footnote{Each such experiment is run many times to reduce variance.}
Accordingly, we have a three-dimensional space of atomic experiments one needs to run and interpret: \{pairs of algorithms\} x \{competition models\} x \{bandit instances\}, and we are looking for findings that are consistent across this entire space. It is essential to keep each of the three dimensions small yet representative. In particular, we need to capture a huge variety of bandit instances with only a few representative examples. Further, one needs succinct and informative summarization of results within one atomic experiment and across multiple experiments (\eg see Table~\ref{sim_table}).


While amenable to simulations, our model appears difficult to analyze. This is for several reasons:
%
intricate feedback loop from performance to reputations to users to performance;
%
mean reputation, most connected to our intuition, is sometimes a bad predictor in competition (see Sections~\ref{sec:isolation} and~\ref{sec:revisited});
%
mathematical tools from regret-minimization would only produce ``asymptotic" results, which do not seem to suffice. Further, we believe that resolving first-order theoretical questions about our model would not add much value to this paper. \asedit{Indeed, we are in the realm of stylized economic models that provide mathematical intuition about the world, and \cite{CompetingBandits-itcs18} already has an elaborate analysis with similar high-level conclusions.}




\subsection{Related work}

\xhdr{Machine learning.} Multi-armed bandits (MAB) is a tractable abstraction for the tradeoff between exploration and exploitation. MAB problems have been studied for many decades, see \cite{Bubeck-survey12,LS19bandit-book} for background on this immense body of work; we only comment on the most related aspects.

We consider MAB with i.i.d. rewards, a well-studied and well-understood MAB model \cite{bandits-ucb1}. We focus on a well-known distinction between ``greedy" (exploitation-only) algorithms, ``naive" algorithms that separate exploration and exploitation, and ``smart" algorithms that combine them. Switching from ``greedy" to ``naive" to ``smart" algorithms involves substantial adoption costs in infrastructure and personnel training \cite{MWT-WhitePaper-2016,DS-arxiv}.

The study of competition vs. exploration has been initiated in \cite{CompetingBandits-itcs18}, as discussed above.
Our setting is also closely related to the ``dueling algorithms" framework \cite{DuelingAlgs-stoc11}, but this framework considers offline / full feedback scenarios whereas we focus on online machine learning problems.


In ``dueling bandits" (e.g., \cite{Yue-dueling-icml09, Yue-dueling12}), an algorithm sets up a ``duel" between a pair of arms in each round, and only learns which arm has ``won". While this setting features a form of competition inside an MAB problem, it is very different from ours.


The interplay between exploration, exploitation and incentives has been studied in other scenarios: incentivizing exploration in a recommendation system,
    \eg \cite{Kremer-JPE14,Frazier-ec14,Che-13,ICexploration-ec15,Bimpikis-exploration-ms17},
dynamic auctions
    (see \cite{DynAuctions-survey10} for background),
online ad auctions, \eg
    \cite{MechMAB-ec09,DevanurK09,NSV08,Transform-ec10-jacm},
human computation
    \cite{RepeatedPA-ec14,Ghosh-itcs13,Krause-www13},
and repeated auctions, \eg
    \cite{Amin-auctions-nips13,Amin-auctions-nips14,Jieming-ec18}.


\xhdr{Economics.}
Our work is related to a longstanding economics literature on competition vs. innovation, \eg \cite{Schumpeter-42,barro2004economic,Aghion-QJE05}. While this literature focuses on R\&D costs of innovation, ``reputational costs" seem new and specific to exploration.


Whether data gives competitive advantage
has been studied theoretically in \cite{varian2018artificial,
  lambrecht2015can} and empirically in
\cite{bajari2018impact}.
While these papers find that small amounts
  of additional data do not provide significant improvement,
 % in training a more predictive model,
  they focus on learning in isolation.
  %, which overlooks certain aspects in a competition.
%\swdelete{These studies consider the effect of additional data in terms of helping towards solving a machine learning problem in isolation and argue that it does not play a large role.}
The first-mover advantage has been well-studied in other settings in economics and marketing, see survey \cite{kerin1992first}.

\asedit{The most common measures of market ``competitiveness" such as the Lerner Index or the Herfindahl-Hirschman Index of a market rely on ex-post observable attributes of a market such as prices or market shares \cite{tirole1988theory}. However, neither is applicable to our setting: in our model, there are no prices, and market shares are endogenous.}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../competing_bandits"
%%% End:
