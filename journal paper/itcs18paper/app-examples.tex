

\newcommand{\ExplorExploit}{\term{ExplorExploit}}
\newcommand{\PhasedExplorExploit}{\term{PhasedExplorExploit}}
%\newcommand{\RandomDynGreedy}{\term{RandomDynGreedy}}
%\newcommand{\SuccesiveElimination}{\term{SuccesiveElimination}}
\newcommand{\SuccesiveEliminationReset}{\term{SuccesiveEliminationReset}}

\newcommand{\IReg}{R^{\term{inst}}} % instantaneous regret

%This appendix provides some pertinent background on multi-armed
%bandits (\emph{MAB}). We discuss \BIR and monotonicity of several MAB algorithms, touching upon: \DynGreedy and \StaticGreedy (Section~\ref{sec:MAB-greedy}), ``naive" MAB algorithms that separate exploration and exploitation (Section~\ref{sec:MAB-naive}), and ``smart" MAB algorithms that combine exploration and exploitation (Section~\ref{sec:MAB-smart}). As we do throughout the paper, we focus on MAB with i.i.d. rewards and a Bayesian prior; we call it \emph{Bayesian MAB} for brevity.


% For a given mean reward vector $\mu$, the $n$-th step instantaneous regret is
%\begin{align}
%\regret(n\mid\mu) &:= \max_{a\in A} \mu_a - \rew(n\mid\mu),\\
%\regretWC(n)    &:=  \sup_{\text{mean reward vectors $\mu$}} \; \BIR(n\mid \mu).
%\end{align}

This appendix provides some auxiliary results on \bmonotonicity of some standard algorithms: \DynGreedy, \DynamicEpsGreedy and \Thompson. The former is needed in Section~\ref{sec:theory}, the last two merely add motivation for our theoretical story. The result on\Thompson also implies that $\BIR(t)\leq \tildeO(t^{-1/2})$. Recall that an algorithm is called \bmonotone if its Bayesian-expected reward is non-decreasing in time.

We consider Bayesian MAB with Bernoulli rewards. There are $T$ rounds and $K$ arms. In each round $t\in [T]$, the algorithm chooses an arm $a_t\in A$ and receives a reward $r_t\in\{0,1\}$ for this arm, drawn from a fixed but unknown distribution. The set of all arms is $A$; mean reward of arm $a$ is denoted $\mu_a$. \asedit{The mean reward vector $\mu = (\mu_a:\; a\in A)$ is drawn from a common Bayesian prior $\priorMu$.}
We let
    $\rew(t) = \mu_{a_t}$
denote the instantaneous mean reward of the algorithm.

%Instantaneous regret is denoted
%    $\IReg(t) := \max_{a\in A} \mu_a - \mu_{a_t}$,
%so that
%    $\BIR(t) = \E[\IReg(t)]$.

%\subsection{Monotone algorithms}
%\label{app:MAB-mono}


%This subsection argues that ``monotonicity" is a mild assumption in our context. To this end, we present versions of some standard MAB algorithms that are monotone and satisfy ``standard" regret bounds.



\xhdr{Monotonicity for the greedy algorithm.}
We state the monotonicity-in-information result for the ``Bayesian-greedy step": informally, exploitation can only get better with more data.  We invoke this result directly in Section~\ref{sec:theory}, and use it to derive monotonicity of \DynGreedy and \DynamicEpsGreedy.

A formal statement needs some scaffolding. The $n$-step history is the random sequence
    $H_n = \rbr{ (a_t,r_t):\, t\in[n]}$.
Realizations of $H_n$ are called \emph{realized histories}.
Let $\mH_n$ be the set of all possible values of $H_n$. The \emph{Bayesian-greedy step} given an $n$-step history $h\in \mH_n$ is defined as
\[ \DG(h) := \argmax_{a\in A} \E\sbr{ \mu_a\mid H_n = h },
\quad\text{ties broken arbitrarily}.\]
(However, recall that such ties are ruled out by Assumption~\ref{eq:assn-distinct}.) Now, the result is as follows:

\begin{lemma}[\citet{ICexplorationGames-ec16}]\label{lm:MII}
Let $h,h'$ be two realized histories such that $h$ is a prefix of $h'$. Then
\[ \E\sbr{ \mu_{\DG(h)} } \leq \E\sbr{ \mu_{\DG(h')} }. \]
\end{lemma}

\begin{corollary}\label{dgmono}
\DynGreedy is \bmonotone. Moreover,
$\E[\rew(n)]$ strictly increases in each time step $n$ with $\Pr[a_n\neq a_{n+1}]>0$.
\end{corollary}

\begin{proof}
Bayesian-monotonicity follows directly. The ``strictly increases" statement holds because the arm chosen in a given round has a strictly largest Bayesian-expected reward for that round.
\end{proof}

\xhdr{Monotonicity for Epsilon-Greedy.} Lemma~\ref{lm:MII} immediately implies monotonicity of \DynamicEpsGreedy, for a generic choice of exploration probabilities. Recall that in each round $t$, \DynamicEpsGreedy algorithm explores uniformly with a predetermined probability $\eps_t$, and ``exploits" with the remaining probability using the Bayesian-greedy step:
    $a_t = \DG(\text{current data})$.


\begin{corollary}\label{cor:mono-eps-greedy}
\DynamicEpsGreedy is \bmonotone whenever probabilities $\eps_t$ are non-increasing.
\end{corollary}

\xhdr{Monotonicity and \BIR of \Thompson.}
Monotonicity follows from \citet{Selke-BE-2020}; the \BIR result is an immediate corollary given that
    $\BReg(t) \leq \tilde{O}(\sqrt{t})$
for all steps $t$ \citep{Russo-MathOR-14}. Neither result has appeared in print, to the best of our knowledge.

\begin{theorem}[\citet{Selke-BE-2020}]\label{thm:TS-mono}
\asedit{Assume the prior $\priorMu$ is independent across arms.} Then \Thompson is \bmonotone. Consequently, it satisfies $\BIR(t)\leq \tildeO(t^{-1/2})$.
\end{theorem}


The theorem follows easily from two observations in  \citet{Selke-BE-2020}, we provide the proof for completeness. First, let us state the algorithm. Let $\Pr^t$ and $\E^t$ denote, resp., Bayesian posterior probability and expectation conditional on the history observed at time $t$. Let $a^*$ be the best arm: $a^*\in \argmax_a \mu_a$. In each round $t$, \Thompson samples arm $a_t$ as follows:
\begin{align}\label{eq:TS-defn}
    \Pr[a_t=a] := {\Pr}^t[a^*=a]
    \quad\text{for each arm $a$}.
\end{align}


\begin{proof}[Proof of Theorem~\ref{thm:TS-mono}]
The two observations from \citet{Selke-BE-2020}  are as follows: for each arm $a$,
\begin{OneLiners}
\item[(i)]
$\E\sbr{ \mu_a \mid a_t = a} \;\Pr[a_t=a] = \E\sbr{H_{t,a}}$,
where
    $H_{t,a} := \E^t[\mu_a]\; \Pr^t[a_t = a]$.

\item[(ii)]
The process $(H_{t,a}:\;t\in\N)$
is a submartingale.
\end{OneLiners}

We use these observations as follows:
\begin{align*}
\E[\rew(t)]
    &= \sum_{\text{arms $a$}}
        \E\sbr{ \mu_a \mid a_t = a} \;\Pr[a_t=a]
    =\sum_{\text{arms $a$}}  \E\sbr{ H_{t,a} }
    \qquad\EqComment{by (i)}\\
\E[\rew(t+1)]
    &= \sum_{\text{arms $a$}}  \E\sbr{ H_{t+1,\;a} }
    \geq \sum_{\text{arms $a$}}  \E\sbr{ H_{t,a} }
    =\E[\rew(t)]
    \qquad\EqComment{by (ii)} \qedhere
\end{align*}
\end{proof} 