This section is dedicated to the \HardMaxRandom response model, where each principal is always chosen with some positive baseline probability. The main technical result for this model states that a principal with asymptotically better \BIR wins by a large margin: after a ``learning phase" of constant duration, all agents choose this principal with maximal possible probability $\respF(1)$. For example, a principal with $\BIR(n)\leq \tilde{O}(n^{-1/2})$ wins over a principal with $\BIR(n)\geq \Omega(n^{-1/3})$. However, this positive result comes with a significant caveat detailed in Section~\ref{sec:random-greedy}.

We formulate and prove a cleaner version of the result, followed by a more general formulation developed in a subsequent Remark~\ref{rem:random-messy}. We need to express a property that \alg[1] eventually catches up and surpasses \alg[2], even if initially it receives only a fraction of traffic. For the cleaner version, we assume that both algorithms are well-defined for an infinite time horizon, so that their \BIR does not depend on the time horizon $T$ of the game.  Then this property can be formalized as:
\begin{align}\label{eq:random-better-clean}
(\forall \eps>0)\qquad
\BIR_1(\eps n)/\BIR_2(n) \to 0.
\end{align}
In fact, a weaker version of \eqref{eq:random-better-clean} suffices:
denoting $\eps_0 = \respF(-1)$, for some constant $n_0$ we have
\begin{align}\label{eq:random-better-weaker}
(\forall n\geq n_0) \qquad
\BIR_1(\eps_0 n/2)/\BIR_2(n) <\tfrac12.
\end{align}
We also need a very mild technical assumption on the ``bad" algorithm:
\begin{align}\label{eq:random-assn}
 (\forall n\geq n_0) \qquad
  \BIR_2(n) > 4\,e^{-\eps_0 n/12}.
\end{align}

\begin{theorem}\label{thm:random-clean}
Assume \HardMaxRandom response function. Suppose both algorithms are monotone and well-defined for an infinite time horizon, and satisfy~\eqref{eq:random-better-weaker} and~\eqref{eq:random-assn}. Then each agent $t\geq n_0$ chooses principal $1$ with maximal possible probability $\respF(1) = 1- \eps_0$.
\end{theorem}

\begin{proof}
Consider global round $t\geq n_0$. Recall that each agent chooses principal $1$ with probability at least
    $\respF(-1)>0$.

Then
    $\E[n_1(t+1)] \geq 2\eps_0\,t $.
By Chernoff Bounds (Theorem~\ref{thm:chernoff}), we have that
    $n_1(t+1)\geq \eps_0 t$
holds with probability at least $1-q$,
where $q = \exp(-\eps_0 t/12)$.

We need to prove that
    $\PMR_1(t) - \PMR_2(t)>0$.
For any $m_1$ and $m_2$, consider the quantity
\[ \Delta(m_1,m_2) := \BIR_2(m_2+1) - \BIR_1(m_1+1).\]
Whenever $m_1 \geq \eps_0 t/2 -1$ and $m_2<t$, it holds that
\begin{align*}
\Delta(m_1,m_2) \geq \Delta(\eps_0 t / 2, t)
    \geq \BIR_2(t)/2.
\end{align*}
The above inequalities follow, resp., from algorithms' monotonicity and \eqref{eq:random-better-weaker}. Now,
\begin{align*}
\PMR_1(t) - \PMR_2(t)
    &= \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}{\Delta(m_1,m_2)} \\
    &\geq -q
        + \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}
            {\Delta(m_1,m_2)\mid m_1 \geq \eps_0 t/2-1} \\
    &\geq \BIR_2(t)/2-q \\
    &> \BIR_2(t)/4 > 0  \EqComment{by \eqref{eq:random-assn}}. \qedhere
\end{align*}
\end{proof}

\begin{remark}\label{rem:random-messy}
  Many standard MAB algorithms in the literature are parameterized by
  the time horizon $T$. Regret bounds for such algorithms usually
  include a polylogarithmic dependence on $T$. In particular, a
  typical upper bound for \BIR has the following form:
  \OMIT{\footnote{We
    provide upper bounds on \BIR for several standard MAB algorithms
    to illustrate these dependencies in the appendix.\sw{added}}}
\begin{align}
    \BIR(n\mid T)\leq \polylog(T)\cdot n^{-\gamma}
    \quad \text{for some $\gamma\in(0,\tfrac12]$}.
\end{align}
Here we write $\BIR(n\mid T)$ to emphasize the dependence on $T$.

We generalize \eqref{eq:random-better-weaker} to handle the dependence
on $T$: there exists a number $T_0$ and a function $n_0(T)\in \polylog(T)$ 
such that 
\begin{align}\label{eq:random-better-messy}
(\forall T\geq T_0,\; n\geq n_0(T)) \quad
\frac{\BIR_1(\eps_0 n /2\mid T)}{\BIR_2(n\mid T)} <\frac12.
\end{align}
If this holds, we say that \alg[1] \emph{BIR-dominates} \alg[2].

We provide a version of Theorem~\ref{thm:random-clean} in which algorithms are parameterized with time horizon $T$ and condition \eqref{eq:random-better-weaker} is replaced with \eqref{eq:random-better-messy}; its proof is very similar and is omitted.
\end{remark}

%\begin{theorem}\label{thm:random-messy}
%Assume \HardMaxRandom response function, and algorithms that satisfy \eqref{eq:random-better-messy} and~\eqref{eq:random-assn}. Then each agent
%    $t\geq n_0(T)$ chooses principal $1$ with probability $\respF(1)$.
%\end{theorem}


%Let \alg be a monotone MAB algorithm and $\mA$ be a finite set of monotone MAB algorithms such that each algorithm in $\mA$ satisfies \eqref{eq:random-assn} and is ``dominated" by \alg in the sense of \eqref{eq:random-better-messy}.
%Assume principals can only choose algorithms from $\mA\cup \{\alg\}$.

To state a game-theoretic corollary of Theorem~\ref{thm:random-clean}, we consider a version of the competition game between the two principals in which they can only choose from a finite set $\mA$ of monotone MAB algorithms. One of these algorithms is ``better" than all others; we call it the \emph{special} algorithm. Unless specified otherwise, it BIR-dominates all other allowed algorithms. The other algorithms satisfy \eqref{eq:random-assn}. We call this game the \emph{restricted competition game}.

\begin{corollary}\label{cor:random}
Assume \HardMaxRandom response function. Consider the restricted competition game with special algorithm \alg. Then, for any sufficiently large time horizon $T$, this game has a unique Nash equilibrium: both principals choose \alg.
\end{corollary}


\subsection{A little greedy goes a long way}
\label{sec:random-greedy}

Given any monotone MAB algorithm other than \DynGreedy, we design a modified algorithm which learns at a slower rate, yet ``wins the game" in the sense of Theorem~\ref{thm:random-clean}. As a corollary, the competition game with unrestricted choice of algorithms typically does not have a Nash equilibrium.

Given an algorithm \alg[1] that deviates from \DynGreedy starting from
step $n_0$ and a ``mixing'' parameter $p$, we will construct a
modified algorithm as follows.
\begin{enumerate}
\item The modified algorithm coincides with \alg[1] (and \DynGreedy)
for the first $n_0-1$ steps;
\item In each step $n\geq n_0$, \alg[1] is invoked with probability
  $1-p$, and with the remaining probability $p$ does the ``greedy
  choice": chooses an action with the largest posterior mean reward
  given the current information collected by \alg[1].
\end{enumerate}
%The modified algorithm is called \emph{greedy deviation}, with probability parameter $p$.
For a cleaner comparison between the two algorithms, the modified algorithm does not record rewards received in steps with the ``greedy choice". Parameter $p>0$ is the same for all steps.

\begin{theorem}\label{thm:random-greedy}
Assume symmetric \HardMaxRandom response function. Let $\eps_0 = \respF(-1)$ be the baseline probability. Suppose \alg[1] deviates from \DynGreedy starting from some step $n_0$. Let \alg[2] be the modified algorithm, as described above, with mixing parameter $p$ such that
    $(1-\eps_0)(1-p)>\eps_0$.
Then each agent $t\geq n_0$ chooses principal $2$ with maximal possible probability $1-\eps_0$.
\end{theorem}

\begin{corollary}\label{cor:random-greedy}
  Suppose that both principals can choose any monotone MAB algorithm, and assume the symmetric \HardMaxRandom response
  function. Then for any time
  horizon $T$, the only possible \emph{pure} Nash equilibrium is one
  where both principals choose \DynGreedy. Moreover, no pure Nash
  equilibrium exists when some algorithm ``dominates" \DynGreedy in
  the sense of \eqref{eq:random-better-messy} and the time horizon $T$
  is sufficiently large.
\end{corollary}


\begin{remark}
The modified algorithm performs exploration % ingests information
at a slower rate. Let us argue how this may translate  into a larger \BIR compared to the original algorithm. Let  $\BIR'_1(n)$ be the \BIR of the ``greedy choice" after after $n-1$ steps of \alg[1]. Then
\begin{align}\label{eq:random-BIR}
\BIR_2(n)
    &= \Ex{m\sim (n_0-1)+\term{Binomial}(n-n_0+1,1-p)}{(1-p) \cdot \BIR_1(m) + p \cdot \BIR'_1(m)}.
\end{align}
In this expression, $m$ is the number of times \alg[1] is invoked in the first $n$ steps of the modified algorithm. Note that
    $\E[m] = n_0-1 + (n-n_0+1)(1-p) \geq (1-p)n$.

Suppose $\BIR_1(n)= \beta n^{-\gamma}$ for some constants $\beta,\gamma>0$. Further, assume 
    $\BIR'_1(n)\geq  c\; \BIR_1(n)$,
for some $c>1-\gamma$.
Then for all $n\geq n_0$ and small enough $p>0$ it holds that:
\begin{align*}
 \BIR_2(n) 
    &\geq  (1-p+pc)\; \E[\; \BIR_1(m) \;] \\
\E[\; \BIR_1(m) \;]
    &\geq \BIR_1(\; \E[m] \;) &\qquad\text{(By Jensen's inequality)} \\
    &\geq \BIR_1(\; (1-p)n \;) &\qquad\text{(since $\E[m]\geq n(1-p)$)}  \\
    &\geq \beta\cdot n^{-\gamma} \cdot (1-p)^{-\gamma} 
        &\qquad\text{(plugging in $\BIR_1(n)=\beta n^{-\gamma}$)}  \\
    &> \BIR_1(n)\;\; (1-p\gamma)^{-1}
        &\qquad\text{(since $(1-p)^\gamma < 1-p\gamma$)}.\\
\BIR_2(n) 
    &>\alpha\cdot \BIR_1(n),
    &\text{where} \quad 
    \alpha = \tfrac{1-p+pc}{1-p\gamma}>1.
\end{align*}
(In the above equations, all expectations are over $m$ distributed as in \eqref{eq:random-BIR}.)
\end{remark}


\begin{proof}[Proof of Theorem~\ref{thm:random-greedy}]
  Let $\rew'_1(n)$ denote the Bayesian-expected reward of the ``greedy
  choice'' after after $n-1$ steps of \alg[1]. Note that
  $\rew_1(\cdot)$ and $\rew'_1(\cdot)$ are non-decreasing: the former
  because \alg[1] is monotone and the latter because the ``greedy
  choice'' is only improved with an increasing set of
  observations. Therefore, the modified algorithm \alg[2] is monotone
  by \eqref{eq:random-BIR}.

  By definition of the ``greedy choice,'' $\rew_1(n)\leq \rew'_1(n)$
  for all steps $n$. Moreover, by Lemma~\ref{lm:DG-rew},
  \alg[1] has a strictly smaller $\rew(n_0)$ compared to \DynGreedy;
  so, $\rew_1(n_0)<\rew_2(n_0)$.

Let $\alg$ denote a copy of \alg[1] that is running ``inside" the modified algorithm \alg[2]. Let $m_2(t)$ be the number of global rounds before $t$ in which the agent chooses principal $2$ \emph{and} \alg is invoked; in other words, it is the number of agents seen by \alg before global round $t$. Let $\mM_{2,t}$ be the agents' posterior distribution for $m_2(t)$.

We claim that in each global round $t\geq n_0$, distribution $\mM_{2,t}$ stochastically dominates distribution $\posteriorN{1}{t}$, and $\PMR_1(t)<\PMR_2(t)$. We use induction on $t$. The base case $t=n_0$ holds because $\mM_{2,t} = \posteriorN{1}{t}$ (because the two algorithms coincide on the first $n_0-1$ steps), and $\PMR_1(n_0)<\PMR_2(n_0)$ is proved as in \eqref{eq:pf:thm:DG-dominance}, using the fact that $\rew_1(n_0)<\rew_2(n_0)$.

The induction step is proved as follows. The induction hypothesis for global round $t-1$ implies that agent $t-1$ is seen by \alg with probability $(1-\eps_0)(1-p)$, which is strictly larger than $\eps_0$, the probability with which this agent is seen by \alg[2]. Therefore, $\mM_{2,t}$ stochastically dominates $\posteriorN{1}{t}$.
\begin{align}
\PMR_1(t)
  &= \Ex{n\sim \posteriorN{1}{t}} {\rew_1(n+1)} \nonumber \\
  &\leq \Ex{m\sim \mM_{2,t}} {\rew_1(m+1)}
    \label{eq:pf:thm:random-greedy-1}\\
  &< \Ex{m\sim \mM_{2,t}} {(1-p)\cdot \rew_1(m+1) + p\cdot \rew'_1(m+1)}
    \label{eq:pf:thm:random-greedy-2} \\
  &= \PMR_2(t). \nonumber
\end{align}
Here inequality \eqref{eq:pf:thm:random-greedy-1} holds because $\rew_1(\cdot)$ is monotone and $\mM_{2,t}$ stochastically dominates $\posteriorN{1}{t}$, and inequality \eqref{eq:pf:thm:random-greedy-2} holds because $\rew_1(n_0)<\rew_2(n_0)$ and $\mM_{2,t}(n_0)>0$.%
\footnote{If $\rew_1(\cdot)$ is strictly increasing, then inequality \eqref{eq:pf:thm:random-greedy-1} is strict, too; this is because $\mM_{2,t}(t-1)>\posteriorN{1}{t}(t-1)$.  }
\end{proof}



%%% Local Variables:
%%% TeX-master: "main.tex"
%%% End: 