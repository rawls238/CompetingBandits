
In this section, we present our theoretical results for the \TheoryModel. While we provide intuition and proof sketches, the detailed proofs are deferred to Appendix~\ref{sec:theory-proofs}.

\subsection{Preliminaries}
\label{sec:theory-prelims}

\xhdr{Notation.} Let $\rew_i(n)$ denote the realized reward of principal $i$ at local step $n$. For a global round $t$, let $n_i(t)$ denote the number of global rounds before $t$ in which principal $i$ is chosen. Note that
    \[ \PMR_i(t):=  \E\sbr{ r_t\mid i_t = i }
        = \E\sbr{ \rew_i(n_i(t)+1) }. \]

\xhdr{Assumptions.} We make two mild assumptions on the prior. First, each arm $a$ can be best:
\begin{align}\label{eq:assn-prob}
\forall a\in A:\;\;\;
\Pr\sbr{\mu_a  > \mu_{a'}
\quad \forall a'\in A}
> 0.
\end{align}

\noindent Second, posterior mean rewards are pairwise distinct given any feasible history $h$:%
\footnote{The \emph{history} of an MAB algorithm at a given step
  comprises the chosen actions and the observed rewards in all
  previous steps. The history is \emph{feasible} if for each arm-reward pair $(a,r)$ in the history, $r$ is in the support of the reward distribution for $a$.}
\begin{align}\label{eq:assn-distinct}
    \E[\mu_a \mid h] \neq \E[\mu_{a'}\mid h] \quad \forall a,a'\in A.
\end{align}
In particular, \emph{prior} mean rewards are pairwise distinct:
$\E[\mu_a] \neq \E[\mu_a']$ for any $a,a'\in A$.

In Appendix~\ref{app:perturb}, we provide two examples for which property \eqref{eq:assn-distinct} is `generic', in the sense that it can be enforced almost surely by a small random perturbation of the prior. The two examples concern, resp., Beta priors and priors with a finite support, and focus on priors $\priorMu$ that are independent across arms.

\xhdr{MAB algorithms.}
We consider two (Bayesian) greedy algorithms. The first one, called  \DynGreedy, chooses an arm $a$ with the largest posterior mean reward $\E[\mu_a \mid \cdot ]$ given all information currently available to the algorithm. The second algorithm, called \StaticGreedy, chooses an arm $a$ with the largest prior mean reward $\E[\mu_a]$, and uses this arm in all rounds. \asdelete{In both algorithms, ties are broken arbitrarily.}


We characterize the inherent quality of an MAB algorithm in terms of its \emph{Bayesian Instantaneous Regret} (henceforth, \BIR), a standard notion from machine learning:
\begin{align}\label{eq:theory-BIR-defn}
\BIR_i(n) := \E\sbr{ \max_{a\in A} \mu_a - \rew_i(n)}.
\end{align}
We are primarily interested in how fast \BIR decreases with $n$. (We treat the number of arms as a constant.) Intuitively, (much) better MAB algorithms tend to have a (much) smaller BIR, see Appendix~\ref{app:bg} for background.

An algorithm is called \emph{\bmonotone} if it can only get better over time: $\rew_i(\cdot)$ is non-decreasing, and therefore $\BIR(\cdot)$ is non-decreasing. This is a mild assumption, see
Appendix~\ref{app:examples}.

%Let $\BIR_i(n) = \BIR(n)$ for the algorithm of principal $i$.


\subsection{Full rationality: \HardMax}
\label{sec:theory-HM}

We posit fully rational agents, in the sense that their response function is \HardMax. We show that principals are not incentivized to \emph{explore}--- \ie to deviate from \DynGreedy. The core technical result is that if one principal adopts \DynGreedy, then the other principal loses all agents as soon as he deviates, in the sense defined below.

\begin{definition}
Two MAB algorithms \emph{deviate} at (local) step $n$ if there is a set $H$ of histories over the previous local steps such that both algorithms lead to $H$ with positive probability, and choose different distributions over arms given any history $h\in H$. The two algorithms deviate \emph{starting from} step $n_0$ if $n=n_0$ is the smallest step when they deviate.
\end{definition}

\begin{theorem}\label{thm:DG-dominance}
Assume \HardMax response function with fair tie-breaking: \asedit{$\respF(0)=\nicefrac{1}{2}$}. Assume that \alg[1] is \DynGreedy, and \alg[2] deviates from \DynGreedy starting from some (local) step $n_0<T$. Then all agents in global rounds $t\geq n_0$ select principal $1$.
\end{theorem}

\asedit{\DynGreedy is a weakly dominant strategy in the competition game, and a unique Nash equilibrium.  This is because \DynGreedy receives, in expectation, at least half of the agents before global round $n_0$, and all agents after that; both are the best possible against \alg[2]. Moreover, \DynGreedy guarantees at least $T/2$ agents in expectation, and any other strategy can receive strictly less than that (\eg if the opponent chooses \DynGreedy).}


\begin{corollary}\label{cor:DG-dominance}
\asedit{\DynGreedy is a weakly dominant strategy in the competition game.} The game has a unique Nash equilibrium: both principals choose \DynGreedy.
\end{corollary}

The proof of Theorem~\ref{thm:DG-dominance} relies on two key lemmas: that deviating from \DynGreedy implies a strictly smaller Bayesian-expected reward, and that \HardMax implies a ``sudden-death" property: if one agent chooses principal $1$ with certainty, so do all subsequent agents. We re-use both lemmas in later results, so we state them in sufficient generality. 

%In particular, Lemma~\ref{lm:DG-rew} works for any response function, as it only considers the stand-alone performance of each algorithm.


\begin{lemma}\label{lm:DG-rew}
Assume that \alg[1] is \DynGreedy, and \alg[2] deviates from \DynGreedy starting from some (local) step $n_0<T$. Then $\rew_1(n_0)>\rew_2(n_0)$. The lemma holds for any response function $\respF$ (as it only considers the stand-alone performance of each algorithm).
\end{lemma}


\begin{lemma}\label{lm:DG-sudden}
Consider \HardMax response function with $\respF(0)\geq\tfrac12$.
Suppose \alg[1] is \bmonotone, and $\PMR_1(t_0)>\PMR_2(t_0)$ for some global round $t_0$. Then $\PMR_1(t)>\PMR_2(t)$ for all subsequent rounds $t$.
\end{lemma}

The remainder of the proof of Theorem~\ref{thm:DG-dominance} uses the conclusion of Lemma~\ref{lm:DG-rew} to derive the precondition for Lemma~\ref{lm:DG-sudden}, \ie goes from $\rew_1(n_0)>\rew_2(n_0)$ to $\PMR_1(n_0)>\PMR_2(n_0)$. The subtlety one needs to deal with is that the principal's ``local" round corresponding to a given ``global" round is a random quantity due to the random tie-breaking.

%\subsection{\HardMax with biased tie-breaking}
%\label{sec:HardMax-biased}

\xhdr{Biased tie-breaking.}
The \HardMax model is very sensitive to tie-breaking \asedit{between the principals}. For starters, if ties are  broken deterministically in favor of principal $1$, then principal 1 can get all agents no matter what the other principal does, simply by using \StaticGreedy.

\begin{theorem}\label{thm:HardMax-hardTies}
Assume \HardMax response function with $\respF(0)=1$ (ties are always broken in favor of principal $1$). If \alg[1] is \StaticGreedy, then all agents choose principal $1$.
\end{theorem}

\begin{proof}[Proof Sketch]
Agent $1$ chooses principal $1$ because of the tie-breaking rule. Since \StaticGreedy is trivially \bmonotone, all the subsequent agents choose principal $1$ by an induction argument similar to the one in the proof of Lemma~\ref{lm:DG-sudden}.
\end{proof}



A more challenging scenario is when the tie-breaking is biased in favor of principal 1, but not deterministically so: $\respF(0)>\tfrac12$. Then this principal also has a ``winning strategy" no matter what the other principal does. Specifically, principal 1 can get all but the first few agents, under a mild technical assumption that \DynGreedy deviates from \StaticGreedy. Principal 1 can use \DynGreedy, or any other \bmonotone MAB algorithm that coincides with \DynGreedy in the first few steps.

%We can generalize the theorem top the case of $q >1/2$ if the
%principal $p_1$ can guarantee better than the a priori best action to
%all the agents following the second.


\begin{theorem}\label{thm:HardMax-biased}
Assume \HardMax response function with $\respF(0)>\tfrac12$ (\ie tie-breaking is biased in favor of principal $1$). Assume the prior $\mP$ is such that \DynGreedy deviates from \StaticGreedy starting from some step $n_0$. Suppose that principal $1$ runs a \bmonotone MAB algorithm that coincides with \DynGreedy in the first $n_0$ steps. Then all agents $t\geq n_0$ choose principal $1$.
\end{theorem}

The proof re-uses Lemmas~\ref{lm:DG-rew} and~\ref{lm:DG-sudden}, which do not rely on fair tie-breaking.

%\ascomment{Steven: can you come up with a short proof sketch / intuition why this thm works?}


%%%%%%%%%%
\subsection{Relaxed rationality: \HardMaxRandom}
\label{sec:theory-HMR}

Consider the \HardMaxRandom response model, where each principal is chosen with some positive baseline probability. Informally, we find that
%\begin{align}\label{eq:theory-HMR-informal}
\emph{a much better algorithm wins big}.
%\end{align}
In more detail, a principal with asymptotically better \BIR wins by a large margin: after a ``learning phase" of constant duration, all agents choose this principal with maximal possible probability $\respF(1)$. For example, a principal with $\BIR(n)\leq \tilde{O}(n^{-1/2})$ prevails over a principal with $\BIR(n)\geq \Omega(n^{-1/3})$.

%However, this positive result comes with a significant caveat detailed in Section~\ref{sec:random-greedy}.

%We formulate and prove a cleaner version of the result, followed by a more general formulation developed in a subsequent Remark~\ref{rem:random-messy}.

To state this result, we need to express a property that \alg[1] eventually catches up and surpasses \alg[2], even if initially it receives only a fraction of traffic. We assume that both algorithms run indefinitely and do not depend on the time horizon $T$; call such algorithms \emph{$T$-oblivious}. In particular, their \BIR does not depend on the time horizon $T$ of the game.  Then this property can be formalized as:
\begin{align}\label{eq:random-better-clean}
(\forall \eps>0)\qquad
\frac{\BIR_1(\eps n)}{\BIR_2(n)} \to 0.
\end{align}
In fact, a weaker version suffices:
denoting $\eps_0 = \respF(-1)$, for some constant $n_0$ we have
\begin{align}\label{eq:random-better-weaker}
(\forall n\geq n_0) \qquad
\frac{\BIR_1(\eps_0\, n/2)}{\BIR_2(n)} <\nicefrac12.
\end{align}
If this holds, we say that \alg[1] \emph{BIR-dominates} \alg[2] starting from (local) step $n_0$.

We also need a mild technical assumption that $\BIR_2(\cdot)$ is not extremely small:
%for some constant $m_0$,
\begin{align}\label{eq:random-assn}
 (\exists m_0\;\forall n\geq m_0) \qquad
  \BIR_2(n) > 4\,e^{-\eps_0\ n/12}.
\end{align}

Thus, the main result is stated as follows:

\begin{theorem}\label{thm:random-clean}
Fix a \HardMaxRandom response function $\respF$. Suppose algorithms \alg[1], \alg[2] are \bmonotone and $T$-oblivious, and \eqref{eq:random-assn} holds. If \alg[1] \emph{BIR-dominates} \alg[2] starting from step $n_0$, then each agent $t\geq \max(n_0,m_0)$ chooses principal $1$ with probability $\respF(1) = 1- \eps_0$ (which is the largest possible selection probability for this response function).
\end{theorem}




%Let \alg be a \bmonotone MAB algorithm and $\mA$ be a finite set of \bmonotone MAB algorithms such that each algorithm in $\mA$ satisfies \eqref{eq:random-assn} and is ``dominated" by \alg in the sense of \eqref{eq:random-better-messy}.
%Assume principals can only choose algorithms from $\mA\cup \{\alg\}$.

%\ascomment{Steven: can you come up with a short proof sketch / intuition why this thm works?}

We'd like to use Theorem~\ref{thm:random-clean} to conclude that a (much) better algorithm prevails in equilibrium. \asedit{We consider a version of the competition game in which the principals are restricted to choosing from a given set of MAB algorithms; the algorithms in this set are called \emph{feasible}.}

%Indeed, this holds if the principals can only choose from a finite set of MAB algorithms.
%\begin{definition}\label{def:restricted-competition}
%A \emph{\FiniteGame} is the competition game between the two principals in which they can only choose from a finite set of MAB algorithms (called \emph{feasible}). All feasible algorithms are assumed to be \bmonotone and $T$-oblivious, and to satisfy \eqref{eq:random-assn}.
%\end{definition}

\begin{corollary}\label{cor:random}
Fix a \HardMaxRandom response function $\respF$. \asedit{Consider the competition game in which all feasible MAB algorithms are $T$-oblivious, \bmonotone, and satisfy \eqref{eq:random-assn} for some fixed $m_0$. Suppose some feasible algorithm \alg  BIR-dominates all other feasible algorithms, starting from some local step $n_0$.} Then, for any sufficiently large time horizon $T$, this game has a unique Nash equilibrium, in which both principals choose algorithm \alg.
\end{corollary}


%\subsection{A little greedy goes a long way}
%\label{sec:random-greedy}

\xhdr{Counterpoint: A little greedy goes a long way.}
Given any \bmonotone MAB algorithm \alg other than \DynGreedy, we design a modified algorithm which ``mixes in" some greedy choices (and consequently learns at a slower rate), yet prevails over \alg in the competition game. Thus, we have a counterpoint to ``much better algorithms win": even under \HardMaxRandom, a better algorithm may lose in competition. The corresponding counterpoint to Corollary~\ref{cor:random} states that non-greedy algorithms are \emph{not} chosen in equilibrium.

The modified algorithm, called the \emph{greedy modification} of \alg  with \emph{mixing parameter} $p\in (0,1)$, is defined as follows. Suppose \alg deviates from \DynGreedy starting from some (local) step $n_0$.
%\begin{enumerate}
%\item
The modified algorithm coincides with \DynGreedy
for the first $n_0-1$ steps.
%\item
In each step $n\geq n_0$, \alg is invoked with probability
  $1-p$, and with the remaining probability $p$ does the ``greedy
  choice": chooses an action with the largest posterior mean reward
  given the current information collected by \alg.
%\item
The data from the ``greedy choice" steps are not recorded.%
\footnote{In other words: the algorithm proceeds as if the ``greedy choice" steps have never happened.  While it is usually more efficient to consider all available data, this modification simplifies analysis.}
This completes the specification of the modified algorithm; note that it is not merely
%another pure strategy in the competition game between the two principals, not 
a mixed strategy that randomizes between \alg and the greedy algorithm.


%    \footnote{In other words: in the subsequent rounds, as far as the modified algorithm is concerned, the ``greedy choice" steps have never happened. While it is usually more efficient to consider all available data, this simplification enables a cleaner comparison between the two algorithms. Also, it makes for  a cleaner setup: otherwise, just to make the modified algorithm well-defined, we'd need to assume that \alg[1] is a mapping from step-$n$ histories to actions, for each step $n$. This is usually OK -- most bandit algorithms have this shape anyway -- but it prohibits \alg[1] to correlate its internal randomness across steps.}

%\end{enumerate}
%The modified algorithm is called \emph{greedy deviation}, with probability parameter $p$.
%Parameter $p>0$ is the same for all steps.

We find that the greedy modification prevails in competition if $p$ is small enough. We focus on \emph{symmetric} response functions: ones that satisfy 
$\respF(x)+\respF(-x)=1$ for any $x\in[0,1]$.

\begin{theorem}\label{thm:random-greedy}
Consider a symmetric \HardMaxRandom response function $\respF$ with baseline probability $\eps_0 = \respF(-1)$. Suppose \alg[1] is \bmonotone, and deviates from \DynGreedy starting from some step $n_0$. Let \alg[2] be the greedy modification of \alg[1] with mixing parameter $p>0$ such that
    $(1-\eps_0)(1-p)>\eps_0$.
Then each agent $t\geq n_0$ chooses principal $2$ with probability $1-\eps_0$ (which is the largest possible).
\end{theorem}

Let us analyze the \BIR of the greedy modification (we will use it for what follows). Use \alg[1] and \alg[2] as in the theorem. For each $n\in\N$, let $M_n$ be the number of times \alg[1] is invoked in the first $n$ steps of \alg[2]. Let \alggr be a hypothetical algorithm which at each step $n$ outputs the ``Bayesian-greedy choice" based on the data collected by \alg[1] in the first $n-1$ steps. Let $\BIRgr(n)$ be the \BIR of this algorithm. Let
    $\mathtt{reg}_2(n) = n\cdot \max_a \mu_a - \rew_2(n)$
be the (frequentist) instantaneous regret of \alg[2]. Then
\begin{align}
\E\sbr{ \mathtt{reg}_2(n) \mid M_n = m}
    &= (1-p) \cdot \BIR_1(m) + p \cdot \BIRgr(m). \nonumber \\
\BIR_2(n)
    &= \E\sbr{ (1-p) \cdot \BIR_1(M_n) + p \cdot \BIRgr(M_n) }.
    \label{eq:BIR-modification}
\end{align}

First, let us show how \alg[1] can be a better algorithm than its greedy modification \alg[2]. We need to assume that \alg[1] is (much) better than the greedy step \alggr if the latter is computed on a fraction of the data. Formally, we posit a convex function $f(\cdot)$ and $m_0>0$ such that
\[ \BIRgr(n) \geq f(n)
    \quad\text{and}\quad
    f\rbr{\E[M_n]} \geq 2\, \BIR_1(n) \quad \forall n\geq m_0 .  \]
Then by \eqref{eq:BIR-modification} and Jensen's inequality, for each step $n\geq m_0$ we have
\[\BIR_2(n)
    \geq  \E\sbr{ \BIRgr(M_n) }
    \geq \E\sbr{ f(M_n) }
    \geq f\rbr{ \E[M_n] }
    \geq 2\,\BIR_1(n). \]

Second, let us argue that \alg[2] is \bmonotone. This follows from \eqref{eq:BIR-modification} since both \alg[1] and \alggr are \bmonotone. The latter follows from a ``monotonicity-in-information" property of the ``greedy step": essentially, it can only get better with more information
(see Lemma~\ref{lm:MII}).

Thus, the greedy modification is beneficial in competition \emph{and} keeps us inside the set of \bmonotone MAB algorithms. Consequently, if the principals are restricted to choosing \bmonotone MAB algorithms, then non-greedy algorithms cannot be chosen in equilibrium.

\begin{corollary}\label{cor:random-greedy}
Fix a symmetric \HardMaxRandom response function $\respF$. \asedit{Consider the competition game in which algorithms are feasible if and only if they are \bmonotone.}%
\footnote{\asedit{More generally, this corollary holds if feasible algorithms are \bmonotone and closed under the greedy modification (\ie the greedy modification of any feasible algorithm is also feasible, for any mixing parameter $p$).}}
\asedit{Then there are no Nash equilibria other than (\DynGreedy,\,\DynGreedy).}
%The latter is not a Nash equilibrium when some algorithm BIR-dominates \DynGreedy and time horizon $T$ is sufficiently large.
\end{corollary}

\asedit{Recall that by Theorem~\ref{thm:random-clean} \DynGreedy cannot be played in equilibrium when it is BIR-dominated by some feasible, $T$-oblivious algorithm and the time horizon $T$ is large enough.}

%These conclusions hold even if each principal's strategy set is a subset of \bmonotone MAB algorithms that is closed under the greedy modification.%
%\footnote{That is, the strategy set of each principal consists of some (but not necessarily all) \bmonotone MAB algorithms, and contains the greedy modification for each algorithms in this set and each mixing parameter $p$.}


%\ascomment{Steven: OLD EXAMPLE. I revised it to clarify what it is supposed to mean. But my new example is better, isn't it?}
%
%\ascomment{The conclusion can't be right: "greedy modification" can only improve the BIR at step $n_0$! Perhaps what we derive is that
%$\BIR_2(n)>\BIR_1(n)$ starting from $n=n_0+1$? }
%
%
%\begin{example}
%Consider \alg[1] and \alg[2] as in Theorem~\ref{thm:random-greedy}.
%Let $\BIRgr(n)$ be the \BIR of the ``greedy choice" after $n-1$ steps of \alg[1]. Assume, for some absolute constants $\beta,\gamma>0$ and $c>1-\gamma$, that
%\begin{align}\label{eq:cor:random-greedy-ex-assumption}
% \tfrac{1}{c}\; \BIRgr(n) \geq \BIR_1(n)= \beta\cdot n^{-\gamma}
% \qquad \text{for all local steps $n$}.
%\end{align}
%Then, for a sufficiently small $p>0$, we have $\BIR_2(n)>\BIR_1(n)$ for all local steps $n\geq n_0$.
%\end{example}
%
%\begin{proof}
%By definition of the modified algorithm we have that
%\begin{align}\label{eq:random-BIR}
%\BIR_2(n)
%    &= \Ex{m\sim (n_0-1)+\term{Binomial}(n-n_0+1,1-p)}{(1-p) \cdot \BIR_1(m) + p \cdot \BIRgr(m)}.
%\end{align}
%In this expression, $m$ is the number of times \alg[1] is invoked in the first $n$ steps of the modified algorithm. Note that
%    $\E[m] = n_0-1 + (n-n_0+1)(1-p) \geq (1-p)n$.
%
%
%Then for all $n\geq n_0$ and small enough $p>0$ it holds that:
%\begin{align*}
% \BIR_2(n)
%    &\geq  (1-p+pc)\; \E[\; \BIR_1(m) \;] \\
%\E[\; \BIR_1(m) \;]
%    &\geq \BIR_1(\; \E[m] \;) &\qquad\text{(By Jensen's inequality)} \\
%    &\geq \BIR_1(\; (1-p)n \;) &\qquad\text{(since $\E[m]\geq n(1-p)$)}  \\
%    &\geq \beta\cdot n^{-\gamma} \cdot (1-p)^{-\gamma}
%        &\qquad\text{(plugging in $\BIR_1(n)=\beta n^{-\gamma}$)}  \\
%    &> \BIR_1(n)\;\; (1-p\gamma)^{-1}
%        &\qquad\text{(since $(1-p)^\gamma < 1-p\gamma$)}.\\
%\BIR_2(n)
%    &>\alpha\cdot \BIR_1(n),
%    &\text{where} \quad
%    \alpha = \frac{1-p(1-c)}{1-p\gamma}>1.
%\end{align*}
%(In the above equations, all expectations are over $m$ distributed as in \eqref{eq:random-BIR}.)
%\end{proof}

%%%%%%%%%
\subsection{\SoftMaxRandom response function}
\label{sec:theory-SoftMax}

For the \SoftMaxRandom model, we derive a ``better algorithm wins" result under a much weaker version of BIR-dominance. We start with a formal definition of \SoftMaxRandom:

\begin{definition}\label{def:SoftMax}
A response function $\respF$ is \SoftMaxRandom if the following conditions hold:
\begin{OneLiners}
\item  $\respF(\cdot)$ is bounded away from $0$ and $1$:
    $\respF(\cdot)\in [\eps,1-\eps]$ for some $\eps\in (0,\tfrac12)$,
\item  the response function
 $\respF(\cdot)$ is ``smooth" around $0$:
 \begin{align}\label{eq:SoftMax-smooth}
 \exists\, \text{constants $\delta_0,c_0,c'_0>0$}
    \qquad \forall x\in [-\delta_0,\delta_0] \qquad
    c_0 \leq \respF'(x) \leq c'_0.
 \end{align}
\item fair tie-breaking: $\respF(0)=\tfrac12$.
\end{OneLiners}
\end{definition}

\begin{remark}
This definition is fruitful when parameters $c_0$ and $c_0'$ are close to $\tfrac12$. Throughout, we assume that \alg[1] is better than \alg[2], and obtain results parameterized by $c_0$. By symmetry, one could assume that \alg[2] is better than \alg[1], and obtain similar results parameterized by $c_0'$.
\end{remark}

For the sake of intuition, let us derive a version of Theorem~\ref{thm:random-clean}, with the same assumptions about the algorithms and essentially the same proof. The conclusion is much weaker, though: we can only guarantee that each agent $t\geq n_0$ chooses principal 1 with probability slightly larger than $\tfrac12$. This is essentially unavoidable in a typical case when both algorithms satisfy $\BIR(n)\to 0$, by Definition~\ref{def:SoftMax}.

\begin{theorem}\label{thm:SoftMax-weak}
Assume \SoftMaxRandom response function.  Suppose algorithms \alg[1], \alg[2] satisfy the assumptions in Theorem~\ref{thm:random-clean}. Then each agent
  $t\geq n_0$ chooses principal $1$ with probability
\begin{align}\label{eq:thm:SoftMax-weak}
     \Pr[i_t = 1]\geq \tfrac12 +  \tfrac{c_0}{4}\; \BIR_2(t).
\end{align}
\end{theorem}

%\begin{proof}[Proof Sketch]
To prove this theorem, we follow the steps in the proof of Theorem~\ref{thm:random-clean} to derive \begin{align*}
\PMR_1(t) - \PMR_2(t)
    &\geq \BIR_2(t)/2 -q,
    \quad \text{where $q = \exp(-\eps_0 t/12)$.}
\end{align*}
This is at least $\BIR_2(t)/4$ by \eqref{eq:random-assn}. Then \eqref{eq:thm:SoftMax-weak} follows by the smoothness condition \eqref{eq:SoftMax-smooth}.
%\end{proof}

\OMIT {%%%%%%%%
We recover a version of Corollary~\ref{cor:random}, if each principal's utility is the number of users (rather than the more general model in \eqref{eq:general-utility}).

\begin{corollary}\label{cor:SoftMax}
Assume that the response function is \SoftMaxRandom, and each principal's  utility is the number of users.
%
%Suppose principals can only choose algorithms from $\mA\cup \{\alg\}$, where $\mA\cup \{\alg\}$ is a finite set of \bmonotone MAB algorithms such that each algorithm in $\mA$ satisfies \eqref{eq:random-assn} and $\BReg(n)\to \infty$, and is ``dominated" by \alg in the sense of \eqref{eq:random-better-messy}.
%
Consider the \FiniteGame with special algorithm \alg, and assume that all other allowed algorithms satisfy $\BReg(n)\to \infty$. Then, for any sufficiently large time horizon $T$, this game has a unique Nash equilibrium: both principals choose \alg.
\end{corollary}
} %%%% \OMIT{

Let us relax the notion of BIR-dominance so that the constant multiplicative factors in \eqref{eq:random-better-weaker}, namely
 $\eps_0/2$ and $\tfrac12$, are replaced by constants that can be arbitrarily close to $1$.

\begin{definition}
Suppose MAB algorithms \alg[1],  \alg[2] are $T$-oblivious. Say that
\alg[1] \emph{weakly BIR-dominates} \alg[2] if there exist absolute  constants $\beta_0, \alpha_0\in (0, 1/2)$ and $n_0\in\N$ such that
 \begin{align}\label{eq:SoftMax-better}
   (\forall n\geq n_0) \quad
   \frac{\BIR_1((1-\beta_0)\, n)}{\BIR_2(n)} <1-\alpha_0.
 \end{align}
 \end{definition}


Now we are ready to state the main result for \SoftMaxRandom:

\begin{theorem}\label{thm:SoftMax-strong}
Assume the \SoftMaxRandom response function. Suppose algorithms \alg[1], \alg[2] are \bmonotone and $T$-oblivious, and \alg[1] weakly-BIR-dominates \alg[2]. Posit mild technical assumptions:
  $\BIR_1(n) \to 0$ and that $\BIR_2$ cannot be extremely small, namely:
\begin{align}\label{eq:SoftMax-assn-strong}
 % (\forall n\geq n(\eps)) \qquad
 %  \BIR_2(n) > e^{-\eps n}. %used to be 4exp( -\eps n/6)
(\exists m_0\; \forall n\geq m_0) \qquad
\BIR_2(n) \geq \frac{4}{\alpha_0}
\exp \left( \frac{-n\,\min\{\eps_0, \nicefrac{1}{8}\}}{12}\right).
\end{align}
Then there
  exists some $t_0$ such that each agent $t\geq t_0$ chooses principal
  $1$ with probability
\begin{align}\label{eq:thm:SoftMax-strong}
     \Pr[i_t = 1]\geq \tfrac12 +  \tfrac14 \,c_0\,\alpha_0\; \BIR_2(t).
\end{align}
\end{theorem}

\begin{proof}[Proof Sketch]
The main idea is that even though \alg[1] may have a
slower rate of learning in the beginning, it will gradually catch up
and surpass \alg[2]. We distinguish two phases. In
the first phase, \alg[1] receives a random agent with probability at
least $\respF(-1) = \eps_0$ in each round. Since $\BIR_1$ tends to 0,
the difference in \BIR{s} between the two algorithms is also
diminishing. Due to the \SoftMaxRandom response function, \alg[1]
attracts each agent with probability at least $1/2 - O(\beta_0)$ after
a sufficient number of rounds. Then the game enters the second phase:
both algorithms receive agents at a rate close to $\tfrac12$, and the
fractions of agents received by both algorithms --- $n_1(t)/t$ and
$n_2(t)/t$ --- also converge to $\tfrac12$. At the end of the second
phase and in each global round afterwards, the counts $n_1(t)$ and
$n_2(t)$ satisfy the weak BIR-dominance condition, in the sense that
they both are larger than $n_0$ and $n_1(t)\geq (1-\beta_0)\; n_2(t)$.
At this point, \alg[1] actually has smaller $\BIR$, which reflected in the {\PMR}s eventually. Accordingly, from then on \alg[1]
attracts agents at a rate slightly larger than $\tfrac12$. We prove
that the ``bump'' over $\tfrac12$ is at least on the order of
$\BIR_2(t)$.
\end{proof}

\asedit{It follows that a weakly-BIR-dominating algorithm prevails in equilibrium.} We need a mild technical assumption that cumulative Bayesian regret (\BReg) tends to infinity. \BReg is a standard notion from the literature (along with \BIR):
\begin{align}\label{eq:SoftMax-BReg}
\BReg(n) := n\cdot \E_{\mu\sim\priorMu}
    \left[ \max_{a\in A} \mu_a\right] -
    \E\sbr{ \sum_{m=1}^n \rew(m)}
    = \sum_{m=1}^n \BIR(m).
\end{align}


\begin{corollary}\label{cor:SoftMax-strong}
Assume \SoftMaxRandom response function. Consider the competition game in which all feasible algorithms are \bmonotone, $T$-oblivious, and satisfy $\BReg(n)\to \infty$.%
\footnote{$\BReg(n)\to \infty$ is a mild non-degeneracy condition, see Appendix~\ref{app:bg} for background.}
Suppose some feasible algorithm \alg weakly-BIR-dominates all others. Then, for any sufficiently large time horizon $T$, there is a unique Nash equilibrium: both principals choose \alg.
\end{corollary}



%If both $\BIR_1()$ and $\BIR_2()$ are of the form $\tilde{\Theta}(n^{-\gamma})$, $\gamma>0$, then the old condition requires $\BIR_1(\cdot)$ to be better by constant multiplicative factor $C$, with $C$ sufficiently large, whereas the new condition allows any $C>1$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

%%%%%%%%%%%%%
\subsection{Economic implications}
\label{sec:theory-welfare}

{We frame our contributions in terms of the relationship between \competitiveness and \rationality on one side, and adoption of better algorithms on the other. Recall that both \competitiveness (of the game between the two principals) and \rationality (of the agents) are controlled by the response function $\respF$.}

\OMIT{ %%%%%%
We frame our contributions in terms of the relationship between \competition and \innovation, \ie between the extent to which the game between the two principals is competitive, and the degree of innovation --- adoption of better that these models incentivize. \Competition is controlled via the response function $\respF$, and \innovation refers to the quality of the technology (MAB algorithms) adopted by the principals. The \competition vs. \innovation relationship is well-studied in the economics literature, and is commonly known to often follow an inverted-U shape, as in \reffig{fig:inverted-U} (see Section~\ref{sec:related-work} for citations). \Competition in our models is closely correlated with \rationality: the extent to which agents make rational decisions, and indeed \rationality is what $\respF$ controls directly.
} %%%%%%%%

\xhdr{Main story.}
Our main story concerns the \FiniteGame between the two principals where one allowed algorithm \alg is ``better" than the others. {We track whether and when \alg is chosen in an equilibrium.} We vary \competitiveness/\rationality by changing the response function from \HardMax (full rationality, very competitive environment) to \HardMaxRandom to  \SoftMaxRandom (less rationality and competition). Our conclusions are as follows:
\begin{OneLiners}
\item Under \HardMax, no innovation: \DynGreedy is chosen over \alg.
\item Under \HardMaxRandom, some innovation:  \alg is chosen as long as it BIR-dominates.
\item Under \SoftMaxRandom, more innovation: \alg is chosen as long as it weakly-BIR-dominates.%
\footnote{This is a weaker condition, the better algorithm is chosen in a broader range of scenarios.}
\end{OneLiners}
These conclusions follow, respectively, from Corollaries~\ref{cor:DG-dominance}, \ref{cor:random} and \ref{cor:SoftMax-strong}. Further, {we consider the uniform choice between the principals. It corresponds to the least amount of rationality and competition, and (when principals' utility is the number of agents) uniform choice provides no incentives to innovate.}%
\footnote{On the other hand, if principals' utility is aligned with agents' welfare, then a monopolist principal is incentivized to choose the best possible MAB algorithm (namely, to minimize cumulative Bayesian regret $\BReg(T)$). Accordingly, monopoly would result in better social welfare than competition, as the latter is likely to split the market and cause each principal to learn more slowly. This is a very generic and well-known effect regarding economies of scale.}
Thus, we have an inverted-U relationship, see \reffig{fig:inverted-U2}.


\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[scale=1]
      \draw[->] (-.5,0) -- (9.5,0) node[above]
        {\qquad\qquad Competitiveness/Rationality};
      \draw[->] (0,-.5) -- (0,3) node[above] {Better algorithm in equilibrium};
      \draw[scale=0.8,domain=0.5:9.5,smooth,variable=\x,blue, line width=0.3mm] plot ({\x},{3.5 - 0.15*(\x - 5)^2});
     \node[below] at (1, 0) {\footnotesize \Uniform};
     \node[below] at (3.9, 0) {\footnotesize \SoftMaxRandom};
     \node[below] at (6, 0) {\footnotesize \HardMaxRandom};
     \node[below] at (8, 0) {\footnotesize \HardMax};
      % \draw[scale=0.5,domain=-3:3,smooth,variable=\y,red]  plot ({\y*\y},{\y});
 \end{tikzpicture}

\caption{The stylized inverted-U relationship in the ``main story".}
\label{fig:inverted-U2}
\end{center}
\end{figure}

\xhdr{Secondary story.}
Let us zoom in on the symmetric  \HardMaxRandom model. {Competitiveness and rationality within this model are controlled by the baseline probability $\eps_0 = \respF(- 1)$, which goes smoothly between the two extremes of \HardMax ($\eps_0=0$) and the uniform choice ($\eps_0=\tfrac12$). Smaller $\eps_0$ corresponds to increased rationality and increased competitiveness.} For clarity, we assume that principal's utility is the number of agents.

We consider the marginal utility of switching to a better algorithm. Suppose initially both principals use some algorithm \alg, and principal 1 ponders switching to another algorithm \alg' which BIR-dominates \alg. {We are interested in the marginal utility of this switch. Then:}

\begin{OneLiners}
\item $\eps_0 = 0$ (\HardMax):~~~~ the marginal utility can be negative if \alg is \DynGreedy.

\item $\eps_0$ near $0$:~~~~ only a small marginal utility can be guaranteed, as it may take a long time for $\alg'$ to ``catch up" with \alg, and hence less time to reap the benefits.

\item ``medium-range" $\eps_0$:~~~~ large marginal utility, as $\alg'$ learns fast and gets most agents.

\item $\eps_0$ near $\tfrac12$:~~~~ small marginal utility, as principal 1 gets most agents for free no matter what.
\end{OneLiners}
The familiar inverted-U shape is depicted in Figure~\ref{fig:inverted-U3}.

\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[scale=1]
      \draw[->] (-.5,0) -- (9.5,0) node[above]  {$\eps_0$};
      \draw[->] (0,-.5) -- (0,3) node[above] {marginal utility};
      \draw[scale=0.8,domain=0.5:9.5,smooth,variable=\x,blue, line width=0.3mm] plot ({\x},{3.5 - 0.15*(\x - 5)^2});
     \node[below] at (.6, 0) {\footnotesize \Uniform};
     \node[above] at (.5, 0) {\footnotesize 0};
     % \node[below] at (3.9, 0) {\footnotesize \SoftMaxRandom};
     % \node[below] at (6, 0) {\footnotesize \HardMaxRandom};
     \node[below] at (7.5,0) {\footnotesize \HardMax};
     \node[above] at (7.5, 0) {\footnotesize 1/2};
      % \draw[scale=0.5,domain=-3:3,smooth,variable=\y,red]  plot ({\y*\y},{\y});
 \end{tikzpicture}

\caption{The stylized inverted-U relationship from the ``secondary story"}
\label{fig:inverted-U3}
\end{center}
\end{figure}


%%%%%%%%%%%%%
\subsection{Extensions}
\label{sec:theory-extensions}

Our theoretical results can be extended beyond the basic model in Section~\ref{sec:model}.


\xhdr{Reward-dependent utility.}
Except for Corollary~\ref{cor:SoftMax-strong}, our results allow a more general notion of principal's utility that can depend on both the market share and agents' rewards. Namely, principal $i$ collects $U_i(r_t)$ units of utility in each global round $t$ when she is chosen (and $0$ otherwise), where $U_i(\cdot)$ is some fixed non-decreasing function with $U_i(0)>0$. In a formula,
%\begin{align}\label{eq:general-utility}
%\textstyle
$U_i := \sum_{t=1}^T\; \indicator{i_t=i}\cdot U_i(r_r)$.
%\end{align}

\xhdr{Time-discounted utility.}
Theorem~\ref{thm:DG-dominance} and Corollary~\ref{cor:DG-dominance} holds under a more general model which allows time-discounting: namely, the utility of each principal $i$ in each global round $t$ is $U_{i,t}(r_t)$ if this principal is chosen, and $0$ otherwise, where $U_{i,t}(\cdot)$ is an arbitrary non-decreasing function with $U_{i,t}(0)>0$.

\xhdr{Arbitrary reward distributions.}
Bernoulli rewards can be extended to arbitrary reward distributions. For each arm $a\in A$ there is a parametric family $\psi_a(\cdot)$ of reward distributions, parameterized by the mean reward. Whenever arm $a$ is chosen, the reward is drawn independently from distribution $\psi_a(\mu_a)$. The prior $\priorMu$ and the distributions $(\psi_a(\cdot)\colon a\in A)$ constitute the (full) Bayesian prior on rewards.% denoted $\prior$.

\xhdr{Beliefs.} Rather than knowing the principals' algorithms $(\alg[1],\alg[2])$, the Bayesian prior $\priorMu$, and the response function $\respF$, agents could have beliefs on these objects that need not be correct. If agents have common ``point beliefs", respectively, algorithms $(\alg[1]',\alg[2]')$ prior $\priorMu'$ and response function $\respF'$,  then all our results carry over with respect to these beliefs.

\xhdr{Limited non-stationarity in $\respF$.}
Different agents can have different response functions, in the following sense. For \HardMaxRandom, our results carry over if each agent $t$ has a \HardMaxRandom response function $\respF$ with parameter $\eps_t\geq \eps_0$. For \SoftMaxRandom, different agents can have different response functions that satisfy Definition~\ref{def:SoftMax} (with the same parameters).

\xhdr{MAB extensions.} Our results carry over, with little or no modification of the proofs, to much more general versions of MAB, as long as it satisfies the i.i.d. property. In each round, an algorithm can see a \emph{context} before choosing an action (as in \emph{contextual bandits}) and/or additional feedback other than the reward after the reward is chosen (as in, \eg \emph{semi-bandits}), as long as the contexts are drawn from a fixed distribution, and the (reward, feedback) pair is drawn from a fixed distribution that depends only on the context and the chosen action. The Bayesian prior $\prior$ needs to be a more complicated object, to make sure that \PMR and \BIR are well-defined. Mean rewards may also have a known structure, such as Lipschitzness, convexity, or linearity; such structure can be incorporated via $\prior$. All these extensions have been studied extensively in the literature on MAB, and account for a substantial segment thereof. Background can be found in any of the recent books on MAB \citep{Bubeck-survey12,slivkins-MABbook,LS19bandit-book}.


\xhdr{\BIR can depend on $T$.}
Many MAB algorithms are parameterized by the time horizon $T$, and their regret bounds usually include $\polylog(T)$. In particular,  a typical regret bound for \BIR is
\begin{align}
    \BIR(n\mid T)\leq \polylog(T)\cdot n^{-\gamma}
    \quad \text{for some $\gamma\in(0,\tfrac12]$}.
\end{align}
Here we write $\BIR(n\mid T)$ to emphasize the dependence on $T$. Accordingly, BIR-dominance can be redefined as follows: there exists a number $T_0$ and a function $n_0(T)\in \polylog(T)$
such that
\begin{align}\label{eq:random-better-messy}
(\forall T\geq T_0,\; n\geq n_0(T)) \quad
\frac{\BIR_1(\eps_0 n /2\mid T)}{\BIR_2(n\mid T)} <\frac12.
\end{align}
%If this holds, we say that \alg[1] \emph{BIR-dominates} \alg[2].
%The proof of this extension is very similar and is omitted.

\noindent Weak BIR-dominance can be redefined similarly.
Theorem~\ref{thm:random-clean} and~\ref{thm:SoftMax-weak} easily extend to these versions.



%\begin{theorem}\label{thm:random-messy}
%Assume \HardMaxRandom response function, and algorithms that satisfy \eqref{eq:random-better-messy} and~\eqref{eq:random-assn}. Then each agent
%    $t\geq n_0(T)$ chooses principal $1$ with probability $\respF(1)$.
%\end{theorem}


%Similarly, weak BIR-dominance can be redefined as follows: there exist some $T_0$, a function
%$n_0(T)\in \polylog(T)$, and constants $\beta_0,\alpha_0\in (0, 1/2)$, such that
%\begin{align}\label{eq:SoftMax-better-weaker}
%(\forall T\geq T_0,  n\geq n_0(T)) \quad
%\frac{\BIR_1((1-\beta_0)\, n\mid T)}{\BIR_2(n\mid T)} <1-\alpha_0.
%\end{align}

%Theorem~\ref{thm:random-clean} and Theorem~\ref{thm:SoftMax-weak} extend to these generalized versions; the easy modifications are omitted.

