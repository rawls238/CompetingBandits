\documentclass[../competing_bandits.tex]{subfiles}
\begin{document}

In this section we present our numerical simulations. As discussed in the Introduction, we focus on the \ExptsModel, whereby each agent chooses the firm with a maximal reputation score, modeled as a sliding window average of its rewards. While we experiment with various MAB instances and parameter settings, we only report on selected, representative experiments. Additional plots and tables are provided in Appendix~\ref{app:expts}. Unless noted otherwise, our findings are based on and consistent with all these experiments.

%In this section we move to the reputation choice variant. Recall that in this variant of the model, instead of making choices between firms based on the Bayesian expected reward, each agent chooses the firm with a maximal reputation score (breaking ties uniformly). The reputation score is simply a sliding window average: an average reward of the last $M$ agents that chose this firm. We focus on numerical investigation of this model instead of analytical characterizations of equilibrium strategies.

\subsection{Experiment setup}
\label{expts-prelims}


%The timing of the model is the same as before where each firm commits to a MAB algorithm before the game starts and uses this algorithm to choose its actions. We focus on i.i.d. Bernoulli rewards: the  reward of each arm $a$ is drawn from $\{0,1\}$ independently with expectation $\mu(a)$. The mean rewards $\mu(a)$ are the same for all rounds and both firms, but initially unknown. However, instead of starting from some prior, we suppose that each firm has a uniform, ``fake", prior and that the initial information set of the firm is given by a ``warm start". Each algorithm receives a ``warm start": additional $T_0$ agents that arrive before the game starts, and interact with the firm as described above. The warm start ensures that each firm has a meaningful reputation and initial prior when competition starts..

\xhdr{Challenges.} An ``atomic experiment" is a competition game between a given pair of bandit algorithms, in a given competition model, on a given 
%instance of a 
multi-armed bandit problem (and each such experiment is run many times to reduce variance). Accordingly, we have a three-dimensional space of atomic experiments one needs to run and interpret: \{pairs of algorithms\} x \{competition models\} x \{bandit \asedit{problems}\}, and we are looking for findings that are consistent across this entire space. It is essential to keep each of the three dimensions small yet representative. In particular, we need to capture a huge variety of bandit algorithms and bandit instances with only a few representative examples. Further, we need a succinct and informative summarization of results within one atomic experiment and across multiple experiments (\eg see Table~\ref{fig:market_share}).

\xhdr{Competition model.} All experiments use \HardMax response function (without mentioning it), except Section~\ref{sec:non_greedy} where we use \HardMaxRandom agents. In some of our experiments, one firm is the ``incumbent" who enters the market before the other (``late entrant"), and therefore enjoys a \emph{first-mover advantage}. Formally, the incumbent enjoys additional $X$ rounds of the ``warm start". We treat $X$ as an exogenous element of the model, and study the consequences for a fixed $X$.

\xhdr{MAB algorithms.} In abstract terms, we posit three types of technology, from ``low" to ``medium" to ``high". Concretely, we consider three essential classes of bandit algorithms: ones that never explicitly explore (\emph{greedy algorithms}), ones that explore without looking at the data (\emph{exploration-separating algorithms}), and ones where exploration gradually zooms in on the best arm (\emph{adaptive-exploration algorithms}). In the absence of competition, these classes are fairly well-understood: greedy algorithms are terrible for a wide variety of problem instances, exploration-separated algorithms learn at a reasonable but mediocre rate across all problem instances, and adaptive-exploration algorithms are optimal in the worst case, and exponentially improve for ``easy" problem instances (see Appendix~\ref{app:bg} for more details).

We look for qualitative differences between these three classes under competition. We take a representative algorithm from each class. Our pilot experiments indicate that our findings do not change substantially if other representative algorithms are chosen.
We use \Thompson (\TS) from the ``adaptive-exploration" algorithms, \DynamicEpsGreedy (\DEG) from the ``exploration-separating" algorithms,%
\footnote{In each round, \DynamicEpsGreedy algorithm explores uniformly with a predetermined probability $\eps$, and ``exploits" with the remaining probability, choosing an arm with maximal posterior mean reward given the current data. We use $\eps=5\%$ throughout. Our pilot experiments show that choosing a different $\eps$ does not qualitatively change the results.}  and \DynGreedy (\DG) algorithm  as in Section~\ref{sec:theory-prelims}. For ease of comparison, all three algorithms are parameterized with the same ``fake" Bayesian prior: namely, the mean reward of each arm is drawn independently from a $\Beta(1,1)$ distribution. Recall that Beta priors with 0-1 rewards form a conjugate family, which allows for simple posterior updates.

%Self-contained background can be found in Appendix~\ref{sec:related-classes}.

\xhdr{MAB instances.}
We consider \asedit{bandit problems} with $K=10$ arms and Bernoulli rewards. \asedit{The \emph{\MRV} $(\mu(a):\; a\in A)$ is initially} drawn from some distribution, termed \emph{MAB instance}. We consider three MAB instances:
\begin{enumerate}
\item \emph{Needle-In-Haystack}: one arm (the ``needle") is chosen uniformly at random. This arm has mean reward $.7$, and the remaining ones have mean reward $.5$.

\item \emph{Uniform instance}: the mean reward of each arm is drawn independently and uniformly from $[\nicefrac{1}{4}, \nicefrac{3}{4}]$.
\item \emph{Heavy-Tail instance}: the mean reward of each arm is drawn independently from $\Beta(.6,.6)$ distribution (which is known to have substantial ``tail probabilities").
\end{enumerate}
We argue that these MAB instances are (somewhat) representative. Consider the ``gap" between the best and the second-best arm, an essential parameter in the literature on MAB. The ``gap" is fixed in Needle-in-Haystack, spread over a wide spectrum of values under the Uniform instance, and is spread but  focused on the large values under the Heavy-Tail instance. We also ran smaller experiments with versions of these instances, and achieved similar qualitative results.

\xhdr{Simulation details.}
For each MAB instance we draw $N = 1000$ \MRVs independently from the corresponding distribution. We use this same collection of \MRVs for all experiments with this MAB instance. For each \MRV we draw a table of realized rewards (\emph{realization table}), and use this same table for all experiments on this \MRV. This ensures that differences in algorithm performance are not due to noise in the realizations but due to differences in the algorithms in the different experimental settings.

More specifically, the realization table is a 0-1 matrix $W$ with $K$ columns which correspond to arms, and $T+T_{\max}$ rows, which correspond to rounds. Here $T_{\max}$ is the maximal duration of the ``warm start" in our experiments, \ie the maximal value of $X+T_0$. For each arm $a$, each value $W(\cdot,a)$ is drawn independently from Bernoulli distribution with expectation $\mu(a)$. Then in each experiment, the reward of this arm in round $t$ of the warm start is taken to be $W(t,a)$, and its reward in round $t$ of the game is $W(T_{\max}+t,a)$.

For the reputation scores, we fix the sliding window size $M = 100$. We found that lower values induced too much random noise in the results, and increasing $M$ further did not make a qualitative difference. Unless otherwise noted, we used $T = 2000$.

\xhdr{Terminology.}
A particular instance of the competition game is specified by the MAB instance and the game parameters, as described above. Recall that firms are interested in maximizing their expected market share at the end of the game.
%Following a standard game-theoretic terminology,
Thus, for a given instance of the game and a given firm, algorithm Alg1 \emph{(weakly) dominates} algorithm Alg2 if Alg1 provides a larger (or equal) expected final market share than Alg2, no matter that the opponent does. An algorithm is a (weakly) dominant strategy for the firm if it (weakly) dominates the other two algorithms.
%This is for a particular MAB instance and a particular selection of the game parameters.

\OMIT{Even with a stylized model, numerical investigation is quite challenging. An ``atomic experiment" is a competition game between a given pair of bandit algorithms, in a given competition model, on a given instance of a multi-armed bandit problem.%
\footnote{Each such experiment is run many times to reduce variance.}
Accordingly, we have a three-dimensional space of atomic experiments one needs to run and interpret: \{pairs of algorithms\} x \{competition models\} x \{bandit instances\}, and we are looking for findings that are consistent across this entire space. It is essential to keep each of the three dimensions small yet representative. In particular, we need to capture a huge variety of bandit instances with only a few representative examples. Further, one needs succinct and informative summarization of results within one atomic experiment and across multiple experiments (\eg see Table~\ref{sim_table}).}

\OMIT{
\xhdr{Running time.}
The simulations are computationally intensive. An experiment on a particular MAB instance comprised multiple runs of the competition game: $N$ mean reward vectors times $9$ pairs of algorithms times three values for the warm start. We used a parallel implementation over a cluster of 12 2.2 GHz CPU cores, with 8 GB RAM per core. Each experiment took about $10$ hours.
}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../competing_bandits"
%%% End:
