We present self-contained background on multi-armed bandits (MAB), to make the paper accessible to researchers who are not experts on MAB. More details can be found in any of the monographs
\citep{Bubeck-survey12,slivkins-MABbook,LS19bandit-book}.

We focus on three algorithm classes, as in Section~\ref{sec:sim}:

\begin{itemize}
\item \emph{Greedy algorithms} that strive to maximize the reward for the next round given the available information. Thus, they always ``exploit" and never explicitly ``explore".
    %\asmargincomment{copied the list from the intro.}

\item \emph{Exploration-separating algorithms}
 that separate exploration and exploitation: essentially, each round is dedicated to one and completely ignores the other.

\item \emph{Adaptive-exploration} algorithms that combine exploration and exploitation, and gradually sway the exploration choices towards more promising alternatives.
\end{itemize}

Below we discuss which algorithms are better than others, and what does it \emph{mean} for one bandit algorithm to be better than another. This is a rather subtle issue, because some algorithms may be better for some problem instances and/or time intervals, and worse for some others. In particular, ``better" algorithms are better in the long run, but could be worse initially.

While we list precise upper and lower bounds on the regret rates, the main goal is to illustrate how the three algorithm classes are separated from one another; the exact results are not essential for this paper. For ease of presentation, we use standard asymptotic notation from computer science: $O(f(t))$ and $\Omega(f(t))$ means at most (resp., at least) $f(n)$, up to constant factors, starting from large enough $t$. Likewise, $\tildeO(f(t))$ notation suppresses the $\polylog(t)$ factors.



\xhdr{Fundamentals.}
We are concerned with the following problem. There are $T$ rounds and $K$ \emph{arms} to choose from. In each round $t\in [T]$, the algorithm chooses an arm and receives a reward $r_t\in[0,1]$ for this arm, drawn from a fixed but unknown distribution.%
\footnote{All ``negative" results (\ie lower bounds on regret) assume reward distributions with constant variance.}
The algorithm's goal is to maximize the total reward.

A standard performance measure is \emph{regret}, defined as the difference in the total expected reward between the algorithm and the best arm. In a formula, regret is
    $T\cdot \max_{\text{arms $a$}}\mu_a
    -  \E\sbr{ \sum_{t\in[T]} r_t }$,
where $\mu_a$ is the mean reward of arm $a$.
Normalized by the best arm, regret allows to compare algorithms across different problem instances.
The primary concern is the asymptotic growth rate of regret as a function of $T$.

The three classes of algorithms perform very differently in terms of regret: adaptive-exploration algorithms are by far the best, greedy algorithms are by far the worst, and exploration-separating ones are in the middle. Adaptive-exploration algorithms achieve optimal regret rates:
    $\tildeO(\sqrt{KT})$
for all problem instances, and simultaneously a vastly improved regret rate of
    $O(\tfrac{K}{\Delta}\log T)$
for all problem instances with $\text{gap}\geq \Delta$ (``easy" instances), without knowing the $\Delta$ in advance
\citep{Lai-Robbins-85,bandits-ucb1,bandits-exp3}.%
\footnote{The \emph{gap} is the difference in mean reward between the best arm and the second-best arm.}
Exploration-separating algorithms can only achieve regret $\tildeO(T^{2/3})$ across all problem instances. They can achieve the ``gap-dependent" regret rate stated above, but \emph{only} if they know the $\Delta$ in advance, and with terrible regret $\Omega(\Delta T)$ for some other problem instances \citep{MechMAB-ec09}. Finally, the greedy algorithm is terrible on a wide variety of problem instances, in the sense that with constant probability it fails to try the best arm even once, and therefore suffers regret $\Omega(T)$
\citep[see Chapter 11.2 in][]{slivkins-MABbook}.


The optimal regret rates are achieved by several adaptive-exploration  algorithms, of which the most known are
Thompson Sampling \citep{Thompson-1933,TS-survey-FTML18},%
\footnote{While Thompson Sampling dates back to 1933 and is probably the best-known bandit algorithm, its regret has not been understood until recently \citep{Shipra-colt12,Kaufmann-alt12,Shipra-aistats13}.}
UCB1 \citep{bandits-ucb1},
and Successive Elimination \citep{EvenDar-icml06}.%
\footnote{A substantial follow-up work on more ``refined" regret rates is not as relevant to this paper.}
These algorithms are very simple to describe. Focus on one round and consider the posterior distribution and/or the confidence interval on each arm's mean reward. Thompson Sampling draws a sample (``score") from each arm's posterior distribution, and picks an arm with the largest score. UCB1 picks an arm with the largest upper confidence bound. Successive Elimination eliminates an arm once it is worse than some other arm with high confidence, and chooses uniformly among the remaining arms.

Exploration-separating algorithms completely separate exploration and exploitation. Ahead of time, each round is either selected for exploration, in which case  the distribution over arms does not depend on the observed data, or it is assigned to exploitation, in which case the data from this round is discarded. The simplest approach, called \emph{Explore-First}, explores uniformly for a predetermined number of rounds, then chooses one arm for ``exploitation" and uses it from then on. A more refined approach, called \emph{Epsilon-Greedy}, explores uniformly in each round with a predetermined probability, and ``exploits" with the remaining probability. Both algorithms, and the associated $\tildeO(T^{2/3})$ regret bounds, have been ``folklore knowledge" for decades. The general definition and lower bounds trace back to \citet{MechMAB-ec09}.%
\footnote{\citet{MechMAB-ec09} consider a closely related, but technically different setting, which can be easily ``translated" into ours (either as a corollary or as another application of the same proof technique).}

% add: this is all more general

\xhdr{Advanced aspects.}
Switching from ``greedy" to ``exploration-separating" to ``adaptive-exploration" algorithms involves substantial adoption costs in infrastructure and personnel training \citep{DS-arxiv}. Inserting exploration into a complex decision-making pipeline necessitates a substantial awareness of the technology and a certain change in mindset, as well as an infrastructure to collect and analyze the data. Adaptive exploration requires the said infrastructure to propagate the data analysis back to the ``front-end" where the decisions are made, and do it on a sufficiently fast and regular cadence. Framing the problem (\eg choosing modeling assumptions and action features) and debugging the machine learning algorithms tend to be quite subtle, too.

The lower bounds mentioned above are fairly typical: while they are usually (and most cleanly) presented as worst-case, the actually hold for a wide variety of problem instances. The $\Omega(\sqrt{T})$ lower bound from \citet{bandits-exp3} can be extended to hold for most problem instances, in the following sense: for each instance $\mI$ there exists a ``decoy instance" $\mI'$ such that any algorithm incurs regret $\Omega(\sqrt{T})$ on at least one of them. The ``gap-dependent" lower bound of
 $\Omega(\tfrac{K}{\Delta}\log T)$
in fact holds for all problem instances and all algorithms that are not \emph{terrible} on the large-gap instances \citep{Lai-Robbins-85}. The $\Omega(T^{2/3})$ lower bound for exploration-separating algorithms in fact applies to all problem instances, as long as the algorithm achieves $\tildeO(T^{2/3})$ regret rate in the worst case \citep{MechMAB-ec09}.%
\footnote{Moreover, there is a tradeoff between the worst-case upper bound on the regret rate and a lower bound that applies for all problem instances \citep[Theorem 4.3 in][]{MechMAB-ec09}.}

Some MAB algorithms, \eg Thompson Sampling, are Bayesian: they input a prior on mean rewards, and attain strong Bayesian guarantees (in expectation over the prior) when the prior is correct. Such algorithms can also be initialized with some simple `fake' priors; in fact, this is how Thompson Sampling can be made to satisfy the optimal regret bounds listed above.

The intuition on (the separation between) the three algorithm classes applies more generally, far beyond the basic MAB model discussed above. In particular, all algorithms that we explicitly mentioned are in fact general algorithmic techniques that are known to extend to a variety of more general MAB scenarios, typically with a similarly stark separation in regret bounds.


The greedy algorithm can perform well \emph{sometimes} in a more general model of \emph{contextual bandits}, where auxiliary payoff-relevant signals, a.k.a. contexts, are observed before each round. This phenomenon has been observed in practice
\citep{practicalCB-arxiv18}, and in theory \citep{kannan2018smoothed,bastani2017exploiting,externalities-colt18} under (very) substantial assumptions. The prevalent intuition is that the diversity of contexts can --- under some conditions and to a limited extent --- substitute for explicit exploration.

\xhdr{Instantaneous regret.}
Cumulative performance measures such as regret are not quite appropriate for our setting, as we need to characterize interactions in particular rounds. Instead, our theoretical results focus on \emph{Bayesian instantaneous regret} (\BIR), as defined in Section~\ref{sec:theory-prelims}. Recall that we posit a Bayesian prior on the \MRVs. In the notation of this appendix, the \BIR is simply:
\begin{align*}%\label{eq:bg-BIR-defn}
\BIR(t) := \E_{\text{prior}}\sbr{ \max_{\text{arms $a$}} \mu_a - r_t}.
\end{align*}
\noindent Note that Bayesian regret (\ie regret in expectation over the prior) is precisely
\begin{align}\label{eq:bg-BReg}
  \BReg(T):=
    \E_{\text{prior}}\sbr{ T\cdot \max_{\text{arms $a$}}\mu_a - \sum_{t=1}^T r_t }
    = \sum_{t=1}^T \BIR(t).
\end{align}
We are primarily interested in how fast \BIR decreases with $t$, treating $K$ as a constant.


The three classes are well-separated in terms of \BIR, much like they are in terms of regret.
%(The precise regret rates are listed below only to illustrate this point.)
\begin{itemize}
\item \DynGreedy has at least a constant \BIR for many reasonable priors (where the constant can depend on $K$ and the prior, but not on $t$). The reason / proof is the same as for regret.

\item Exploration-separating algorithms can achieve
    $\BIR(t) = \tildeO\rbr{t^{-1/3}}$
for all priors, \eg by using Epsilon-Greedy algorithm with exploration probability $\eps_t = t^{-1/3}$ in each round $t$.
In the typical scenario when
 $\BReg(t)\geq \Omega(t^{2/3})$,
the \BIR rate of $t^{-1/3}$  cannot be improved by \eqref{eq:bg-BReg}, in the following sense:
if
    $\BIR(t) = \tildeO\rbr{t^{-\gamma}}$
for all $t$, then $\gamma\geq \nicefrac{1}{3}$.

\item Adaptive-exploration algorithms \emph{can} have an even better regret rate: $\BIR(t) = \tildeO\rbr{ t^{-1/2} } $. This holds for Successive Elimination \citep{EvenDar-icml06} and for Thompson Sampling (see Appendix~\ref{app:examples}).%
    \footnote{However, such result is not known for UCB1 algorithm, to the best of our knowledge.}
    Any optimal MAB algorithm enjoys this regret rate ``on average" by \eqref{eq:bg-BReg}, since
        $\BReg(T)\leq \tildeO(\sqrt{T})$.
    In particular, if such algorithm satisfies
        $\BIR(t) = \tildeO\rbr{t^{-\gamma}}$
    for all rounds $t$ and some constant $\gamma$, then $\gamma \leq \nicefrac{1}{2}$.

\end{itemize}

\noindent This theoretical intuition is supported by our numerical simulations: see
Figure~\ref{relative_rep_plots} and Appendix~\ref{app:isol}.


%\xhdr{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../journal_main"
%%% End:
