We present sufficient background on the three classes of MAB algorithms listed in Section~\ref{sec:intro-model}: from adaptive-exploration to exploration-separating to greedy.

\xhdr{Fundamentals.}
We are concerned with the following problem. There are $T$ rounds and $K$ \emph{arms} to choose from. In each round $t\in [T]$, the algorithm chooses an arm and receives a reward $r_t\in[0,1]$ for this arm, drawn from a fixed but unknown distribution. Algorithm's goal is to maximize the total reward.

A standard performance measure is \emph{regret}, defined as the difference in the total expected reward between the algorithm and the best arm.%
\footnote{In a formula, regret is
    $T\cdot \max_{\text{arms $a$}}\mu_a
    -  \E\sbr{ \sum_{t\in[T]} r_t }$,
where $\mu_a$ is the mean reward of arm $a$.}
Since regret is normalized by the best arm, it allows to compare algorithms across different problem instances.
The primary concern is the asymptotic growth rate of regret as a function of the time horizon $T$.

The three classes of algorithms perform very differently in terms of regret. Adaptive-exploration algorithms achieve optimal regret rates:
    $\tildeO(\sqrt{KT})$
for all problem instances, and an improved regret rate of
    $O(\tfrac{K}{\Delta}\log T)$
for problem instances with $\text{gap}\geq \Delta$
\citep{Lai-Robbins-85,bandits-ucb1,bandits-exp3}.%
\footnote{The \emph{gap} is the difference in mean reward between the best arm and the second-best arm. As usual in computer science, the $O(\cdot)$ and $\tildeO(\cdot)$ notation suppresses the constant and
$\polylog(T)$ factors, respectively.}
Exploration-separating algorithms can only achieve regret $\tildeO(T^{2/3})$ across all problem instances. Finally, the greedy algorithm is terrible on a wide variety of problem instances, in the sense that it fails to try the best arm and therefore suffers regret linear in $T$
\citep[see Chapter 11.2 in][]{slivkins-MABbook}.


The optimal regret rates are achieved by many adaptive-exploration  algorithms, of which the most known are
Thompson Sampling \citep{Thompson-1933,TS-survey-FTML18},%
\footnote{While Thompson Sampling is the first and arguably best-known bandit algorithm, the corresponding regret bounds have been proved relatively recently \citep{Shipra-colt12,Kaufmann-alt12,Shipra-aistats13}.}
UCB1 \citep{bandits-ucb1},
and Successive Elimination \citep{EvenDar-icml06}.
Further work on refining these regret rates is not as relevant.

Exploration-separating algorithms completely separate exploration and exploitation. The simplest approach, called \emph{Explore-First}, is to explore uniformly for a predetermined number of rounds, then choose one arm and ``exploit". A slightly more refined approach, called \emph{Epsilon-Greedy}, explores uniformly in each round $t$ with predetermined probability $\eps_t$, and exploits with the remaining probability. A formal definition is that each round is either selected exploration, in which case  the distribution over arms does not depend on data, or it is assigned to exploitation, in which case the data from this round is discarded. The $T^{2/3}$ lower bound on regret follows from a somewhat more general lower bound in \citet{MechMAB-ec09}.

\xhdr{Advanced aspects.}
Switching from ``greedy" to ``exploration-separating" to ``adaptive-exploration" algorithms involves substantial adoption costs in infrastructure and personnel training \citep{DS-arxiv}. Inserting exploration in a complex decision pipeline necessitates a substantial awareness of the technology and a certain change in a mindset, as well as an infrastructure to collect and analyze the data. Adaptive exploration requires the said infrastructure to propagate the data analysis back to the ``front-end" where the decisions are made, and do it on a sufficiently fast and regular cadence. Framing the problem in terms of the appropriate modeling assumptions and action features, and debugging the machine learning code, tend to be quite subtle, too.

The greedy algorithm can perform well \emph{sometimes} in a more general model of \emph{contextual bandits}, where auxiliary payoff-relevant signals, a.k.a. contexts, are observed before each round. This phenomenon has been observed in practice
\citep{practicalCB-arxiv18}, and in theory \citep{kannan2018smoothed,bastani2017exploiting,externalities-colt18} under (very) substantial assumptions. The prevalent intuition is that the diversity of contexts can --- under some conditions and to a limited extent --- substitute for explicit exploration.

\xhdr{Instantaneous regret.}

Let us connect \BIR with the three classes of algorithms listed in Section~\ref{sec:intro-model}. \DynGreedy has at least a constant \BIR for many reasonable priors (where the constant can depend on $K$ and the prior, but not on $n$). \StaticGreedy typically has at least a constant \BIR, in the same sense. Exploration-separating algorithms can have
    $\BIR(n) \sim n^{-1/3}$
for all priors, whereas adaptive-exploration algorithms can have an even better rate
    $\BIR(n) \sim n^{-1/3}$
for all priors. Both rates are up to $\log(n)$ factors, and are the best possible \BIR rates that can be achieved for all priors. More details on this can be found in Appendix~\ref{app:examples}.



\ascomment{stopped here.}

%\xhdr{}
