\xhdr{Bandits and exploration.} Multi-armed bandits (\emph{MAB}) is an elegant and tractable abstraction for tradeoff between \emph{exploration} and \emph{exploitation}: essentially, between acquisition and usage of information. MAB problems have been studied for many decades \asedit{by researchers from computer science, operations research, statistics and economics. The literature on MAB is extremely vast and multi-threaded. The most relevant thread studies a basic model of MAB with IID rewards and no auxiliary structure, see Section~\ref{sec:related-classes}. The basic model has been extended in many different directions, with a considerable amount of work on each: \eg payoffs with a specific structure (\eg combinatorial, linear, convex or Lipschitz), payoff distributions that change over time, and auxiliary payoff-relevant signals.} This literature is covered in many books and surveys. Particularly, see
\citet{Bubeck-survey12,slivkins-MABbook,LS19bandit-book} for background on regret-minimizing formulations, \citet{Gittins-book11} for Bayesian and Markovian formulations, and \citet{CesaBL-book,slivkins-MABbook} for connections to economics and game theory. A discussion of industrial applications of MAB can be found in \citep{DS-arxiv}.


The three-way tradeoff between exploration, exploitation and incentives has been studied in several settings other than ours:
incentivizing exploration in a recommendation system
    \citep[\eg][]{Che-13,Frazier-ec14,Kremer-JPE14,ICexploration-ec15,Bimpikis-exploration-ms17,Bahar-ec16,Jieming-unbiased18},
dynamic auctions
    \cite[\eg][]{AtheySegal-econometrica13,DynPivot-econometrica10,Kakade-pivot-or13},
pay-per-click ad auctions with unknown click probabilities
    \cite[\eg][]{MechMAB-ec09,DevanurK09,Transform-ec10-jacm},
coordinating search and matching by self-interested agents
    \citep{Bobby-Glen-ec16},
as well as human computation
    \cite[\eg][]{RepeatedPA-ec14,Ghosh-itcs13,Krause-www13}.
\citet{Bolton-econometrica99,Keller-econometrica05,Johari-ec12} studied models with self-interested agents jointly performing exploration, with no principal to coordinate them.

There is a superficial similarity --- in name only --- between this paper and the line of work on ``dueling bandits"
    \citep[\eg][]{Yue-dueling12,Yue-dueling-icml09}.
The latter is not about competing bandit algorithms, but rather about scenarios where in each round two arms are chosen to be presented to a user, and the algorithm only observes which arm has ``won the duel".

Our setting is closely related to the ``dueling algorithms" framework of \citet{DuelingAlgs-stoc11}, which studies competition between two principals, each running an algorithm for the same problem. However, \citet{DuelingAlgs-stoc11} consider algorithms for offline / full input scenarios, \asedit{and posit binary (win/lose) payoffs for the principals. Similarly, \citet{ben2017best, ben2019regression} study competition between offline learning algorithms. Whereas we focus on bandit algorithms and competition for users.}

\xhdr{Other related work in economics.} The competition vs. innovation relationship and the inverted-U shape thereof have been introduced in a classic book \citep{Schumpeter-42}, and remained an important theme in the literature ever since \cite[\eg][]{Aghion-QJE05,Vives-08}. Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm.

\gaedit{There is a literature that studies the interaction between learning-by-doing and competition \citep{fudenberg1983learning, dasgupta1988learning, cabral1994learning}. In these models, firms learn while competing against each other where a firm attracting more consumers reduces its per-unit production costs. However, our model focuses on firms learning product quality (as opposed to reducing production costs) and the impact that this learning has on attracting consumers.}

A line of work on \emph{platform competition}, starting with \cite{Rysman09}, concerns competition between firms (\emph{platforms}) that improve as they attract more users (\emph{network effect}); see \citet{Weyl-White-14} for a recent survey. This literature is not concerned with \innovation, and typically models network effects exogenously, whereas in our model network effects are endogenous: they are created by MAB algorithms, an essential part of the model.
\gaedit{A nascent literature studies whether and when network effects manifest themselves in data-intensive markets \citep{prufer2017competing, hagiu2020data}, but typically models learning as a reduced-form function of past consumer history and focus on the role of prices.}
%as opposed to the reputational consequences of learning.

\cite{schmalensee1982product, bagwell1990informational} investigate how buyer uncertainty about product quality can serve as a barrier to entry for late arrivers. We observe a similar effect when we investigate the role that reputation can serve as a barrier to entry. However, in our model this effect is further strengthened by the fact that the firms have to learn while competing adding that the incumbent may not only have a reputational advantage, but additionally has a further advantage due to the data it acquires in the incumbency period. Thus, our model also highlights the role that data can serve as a barrier to entry in online markets which has similarly been noted in \cite{de2020data}. \gaedit{For an extensive overview of the other channels through which first-mover advantages can lead to a competitive advantage, see \cite{kerin1992first}.}


Relaxed versions of rationality similar to ours are found in several notable lines of work. For example, ``random agents" (a.k.a. noise traders) can side-step the ``no-trade theorem'' \citep{Milgrom-Stokey-82}, a famous impossibility result in financial economics. The \SoftMaxRandom model is closely related to the literature on \emph{product differentiation}, starting from \cite{Hotelling-29}, see \cite{Perloff-Salop-85} for a notable later paper. There is a large literature on non-existence of equilibria due to small deviations   (which is related to the corresponding result for \HardMaxRandom), starting with \cite{Rothschild-Stiglitz-76} in the context of health insurance markets. Notable recent papers \citep{Veiga-Weyl-16,Azevedo-Gottlieb-17} emphasize the distinction between \HardMax and versions of \SoftMaxRandom.

We use first-mover advantage and relaxed versions of rationality to model varying competition, instead of classic ``market competitiveness" measures such as the Lerner Index or the Herfindahl-Hirschman Index
\citep{tirole1988theory}. The latter measures rely on ex-post observable attributes of a market such as prices or market shares. Neither is applicable to our setting, since there are no prices, and market shares are endogenous.

\ascomment{Guy: slightly reworded the previous para.}


% moved this thought to the intro.
\OMIT{While agents' rationality and severity of competition are often modeled separately in the literature, it is not unusual to have them modeled with the same ``knob" \cite[\eg][]{Gabaix-16}.}


\subsection{Background for non-specialists: the three classes of algorithms [IN PROGRESS]}
\label{sec:related-classes}

We present sufficient background on the three classes of MAB algorithms listed in Section~\ref{sec:intro-model}: from adaptive-exploration to exploration-separating to greedy.

\xhdr{Fundamentals.}
We are concerned with the following problem. There are $T$ rounds and $K$ \emph{arms} to choose from. In each round $t\in [T]$, the algorithm chooses an arm and receives a reward $r_t\in[0,1]$ for this arm, drawn from a fixed but unknown distribution. Algorithm's goal is to maximize the total reward.

A standard performance measure is \emph{regret}, defined as the difference in the total expected reward between the algorithm and the best arm.%
\footnote{In a formula, regret is
    $T\cdot \max_{\text{arms $a$}}\mu_a 
    -  \E\sbr{ \sum_{t\in[T]} r_t }$,
where $\mu_a$ is the mean reward of arm $a$.}
Since regret is normalized by the best arm, it allows to compare algorithms across different problem instances. 
The primary concern is the asymptotic growth rate of regret as a function of the time horizon $T$.

The three classes of algorithms perform very differently in terms of regret. Adaptive-exploration algorithms achieve optimal regret rates:
    $\tildeO(\sqrt{KT})$
for all problem instances, and an improved regret rate of
    $O(\tfrac{K}{\Delta}\log T)$
for problem instances with $\text{gap}\geq \Delta$
\citep{Lai-Robbins-85,bandits-ucb1,bandits-exp3}.%
\footnote{The \emph{gap} is the difference in mean reward between the best arm and the second-best arm. As usual in computer science, the $O(\cdot)$ and $\tildeO(\cdot)$ notation suppresses the constant and
$\polylog(T)$ factors, respectively.}
Exploration-separating algorithms can only achieve regret $\tildeO(T^{2/3})$ across all problem instances. Finally, the greedy algorithm is terrible on a wide variety of problem instances, in the sense that it fails to try the best arm and therefore suffers regret linear in $T$
\citep[see Chapter 11.2 in][]{slivkins-MABbook}.


The optimal regret rates are achieved by many adaptive-exploration  algorithms, of which the most known are
Thompson Sampling \citep{Thompson-1933,TS-survey-FTML18},%
\footnote{While Thompson Sampling is the first and arguably best-known bandit algorithm, the corresponding regret bounds have been proved relatively recently \citep{Shipra-colt12,Kaufmann-alt12,Shipra-aistats13}.}
UCB1 \citep{bandits-ucb1},
and Successive Elimination \citep{EvenDar-icml06}.
Further work on refining these regret rates is not as relevant.

Exploration-separating algorithms completely separate exploration and exploitation. The simplest approach, called \emph{Explore-First}, is to explore uniformly for a predetermined number of rounds, then choose one arm and ``exploit". A slightly more refined approach, called \emph{Epsilon-Greedy}, explores uniformly in each round $t$ with predetermined probability $\eps_t$, and exploits with the remaining probability. A formal definition is that each round is either selected exploration, in which case  the distribution over arms does not depend on data, or it is assigned to exploitation, in which case the data from this round is discarded. The $T^{2/3}$ lower bound on regret follows from a somewhat more general lower bound in \citet{MechMAB-ec09}.


\ascomment{stopped here.}

\xhdr{Advanced aspects.}
Switching from ``greedy" to ``exploration-separating" to ``adaptive-exploration" algorithms involves substantial adoption costs in infrastructure and personnel training \citep{DS-arxiv}.

A recent line of work \citep{kannan2018smoothed,bastani2017exploiting,externalities-colt18}
finds that the greedy algorithm performs well (in theory), in a model of linear contextual bandits under substantial assumptions on smoothing or heterogeneity of the context vectors.

%\xhdr{}

%%% Local Variables:
%%% TeX-master: "main.tex"
%%% End: 