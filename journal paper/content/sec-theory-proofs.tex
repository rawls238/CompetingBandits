
\xhdr{Some notation.} Without loss of generality, we label actions as $A=[K]$ and sort them according to their prior mean rewards, so that
    $ \E[\mu_1] > \E[\mu_2] > \ldots > \E[\mu_K]$.

Fix principal $i\in \{1,2\}$ and (local) step $n$. The arm chosen by algorithm \alg[i] at this step is denoted $a_{i,n}$, and the corresponding \BIR is denoted $\BIR_i(n)$. History of \alg[i] up to this step is denoted $H_{i,n}$.

Fix agent $t$. Recall that $n_i(t)$ denotes the number of global rounds before $t$ in which principal $i$ is chosen. Let $\posteriorN{i}{t}$ denote the distribution of $n_i(t)$.

Write
    $\PMR(a\mid E) = \E[\mu_a \mid E]$
for posterior mean reward of action $a$ given event $E$.

\xhdr{Chernoff Bounds.}
We use an elementary concentration inequality known as {\em Chernoff Bounds}, in a formulation from~\cite{MitzUpfal-book05}.
\begin{theorem}[Chernoff Bounds]
\label{thm:chernoff}
Consider $n$ i.i.d. random variables $X_1 \ldots X_n$ with values in $[0,1]$. Let
    $X = \tfrac{1}{n} \sum_{i=1}^n X_i$ be their average, and let $\nu = \E[X]$. Then:
\[ \min\left(\; \Pr[ X-\nu > \delta \nu ],\quad
                \Pr[ \nu-X > \delta \nu ]
    \; \right)
    < e^{-\nu n \delta^2/3}
    \quad \text{for any $\delta\in (0,1)$.}
\]
\end{theorem}






\subsection{Main result on \HardMax: Proof of Theorem~\ref{thm:DG-dominance}}
\label{sec:proofs-HM-main}


\begin{proof}[Proof of Lemma~\ref{lm:DG-rew}]
Since the two algorithms coincide on the first $n_0-1$ steps, it follows by symmetry that histories $H_{1,n_0}$ and $H_{2,n_0}$ have the same distribution. We use a \emph{coupling argument}: w.l.o.g., we assume the two histories coincide,
    $H_{1,n_0} = H_{2,n_0} = H$.

At local step $n_0$, \DynGreedy chooses an action $a_{1,n_0} = a_{1,n_0}(H)$
which maximizes the posterior mean reward given history $H$: for any realized history $h\in \support(H)$ and any action $a\in A$
\begin{align}\label{eq:lm:DG-rew-1}
 \PMR(a_{1,n_0} \mid H = h) \geq \PMR(a \mid H=h).
\end{align}

By assumption \eqref{eq:assn-distinct}, it follows that
\begin{align}\label{eq:lm:DG-rew-2}
 \PMR(a_{1,n_0} \mid H = h) > \PMR(a \mid H=h)
 \quad \text{for any $h\in \support(H)$ and $a\neq a_{1,n_0}(h)$.}
\end{align}

Since the two algorithms deviate at step $n_0$, there is a set $S\subset \support(H)$ of step-$n_0$ histories such that $\Pr[S]>0$ and any history $h\in S$ satisfies
    $\Pr[a_{2,n_0}\neq a_{1,n_0} \mid H=h]>0$.
Combining this with \eqref{eq:lm:DG-rew-2},
\begin{align}\label{eq:lm:DG-rew-3}
 \PMR(a_{1,n_0} \mid H = h) > \E\left[ \mu_{a_{2,n_0}}\mid H=h \right]
 \quad\text{for each history $h\in S$}.
\end{align}
Using \eqref{eq:lm:DG-rew-1} and \eqref{eq:lm:DG-rew-3} and integrating over realized histories $h$, we obtain
    $\rew_1(n_0) > \rew_2(n_0)$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lm:DG-sudden}]
Let us use induction on round $t\geq t_0$, with the base case $t=t_0$. Let $\mN=\mN_{1,t_0}$ be the agents' posterior distribution for $n_{1,t_0}$, the number of global rounds before $t_0$ in which principal $1$ is chosen. By induction, all agents from $t_0$ to $t-1$ chose principal $1$, so $\PMR_2(t_0)= \PMR_2(t)$. Therefore,
\[ \PMR_1(t)
    = \Ex{n\sim \mN}{\rew_1(n+1+t-t_0)}
    \geq \Ex{n\sim \mN}{\rew_1(n+1)}
    =\PMR_1(t_0) > \PMR_2(t_0)= \PMR_2(t), \]
where the first inequality holds because \alg[1] is \bmonotone, and the second one is the base case.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:DG-dominance}]
Since the two algorithms coincide on the first $n_0-1$ steps, it follows by symmetry that $\rew_1(n) = \rew_2(n)$ for any $n< n_0$.
By Lemma~\ref{lm:DG-rew},
    $\rew_1(n_0) > \rew_2(n_0)$.

Recall that $n_i(t)$ is the number of global rounds $s<t$ in which principal $i$ is chosen, and $\posteriorN{i}{t}$ is the agents' posterior distribution for this quantity. By symmetry, each agent $t<n_0$ chooses a principal uniformly at random. It follows that
    $\posteriorN{1}{n_0} = \posteriorN{2}{n_0}$
(denote both distributions by $\mN$ for brevity), and $\mN(n_0-1)>0$.
Therefore:
\begin{align}
\PMR_1(n_0)
  &= \Ex{n\sim \mN} {\rew_1(n+ 1)}
   = \sum_{n = 0}^{n_0-1} \mN(n) \cdot{\rew_1(n + 1)} \nonumber \\
  & > \mN(n_0-1)\cdot {\rew_2(n_0)} + \sum_{n = 0}^{n_0-2}  \mN(n)\cdot{\rew_2(n + 1)}
    \nonumber \\
  &= \Ex{n\sim \mN}{\rew_2(n + 1)} = \PMR_2(n_0) \label{eq:pf:thm:DG-dominance}
\end{align}
So, agent $n_0$ chooses principal $1$. By Lemma~\ref{lm:DG-sudden} {(noting that \DynGreedy is \bmonotone)}, all subsequent agents choose principal $1$, too.
\end{proof}

%%%%%%%%%%%%
\subsection{\HardMax with biased tie-breaking:
Proof of Theorem~\ref{thm:HardMax-biased}}
\label{sec:proofs-HM-main}

The proof re-uses Lemmas~\ref{lm:DG-rew} and~\ref{lm:DG-sudden}, which do not rely on fair tie-breaking.

Recall that $i_t$ is the principal chosen in a given global round $t$.
Because of the biased tie-breaking,
\begin{align}\label{eq:thm:HardMax-biased-PMRtoPr}
\text{if $\PMR_1(t)\geq \PMR_2(t)$ then $\Pr[i_t=1]>\tfrac12$.}
\end{align}

Let $m_0$ be the first step when \alg[2] deviates from \DynGreedy, or \DynGreedy deviates from \StaticGreedy, whichever comes sooner. {Then \alg[2], \DynGreedy and \StaticGreedy coincide on the first $m_0-1$ steps. Moreover, $m_0\leq n_0$ (since \DynGreedy deviates from \StaticGreedy at step $n_0$), so \alg[1] coincides with \DynGreedy on the first $m_0$ steps.

So, $\rew_1(n)=\rew_2(n)$ for each step $n<m_0$, because \alg[1] and \alg[2] coincide on the first $m_0-1$ steps. Moreover, if \alg[2] deviates from \DynGreedy at step $m_0$ then
    $\rew_1(m_0) > \rew_2(m_0)$
by Lemma~\ref{lm:DG-rew}; else, we trivially have
    $\rew_1(m_0) = \rew_2(m_0)$.} To summarize:
\begin{align}\label{eq:thm:HardMax-biased-rew}
    \rew_1(n)\geq\rew_2(n)\quad \text{for all steps $n\leq m_0$}.
\end{align}

We claim that $\Pr[i_t=1]>\tfrac12$ for all global rounds $t\leq m_0$. We prove this claim using induction on $t$. The base case $t=1$ holds by \eqref{eq:thm:HardMax-biased-PMRtoPr} and the fact that in step 1, \DynGreedy chooses the arm with the highest prior mean reward. For the induction step, we assume that $\Pr[i_t=1]>\tfrac12$ for all global rounds $t<t_0$, for some $t_0\leq  m_0$. It follows that distribution $\posteriorN{1}{t_0}$ stochastically dominates distribution $\posteriorN{2}{t_0}$.%
\footnote{For random variables $X,Y$ on \R, we say that $X$ \emph{stochastically dominates} $Y$ if $\Pr[X\geq x] \geq \Pr[Y\geq x]$ for any $x\in \R$.}
Observe that
\begin{align}\label{eq:thm:HardMax-biased-PMR-aux}
\PMR_1(t_0)
  = \Ex{n\sim \posteriorN{1}{t_0}} {\rew_1(n+1)}
  \geq \Ex{n\sim \posteriorN{2}{t_0}} {\rew_2(n+1)}
  = \PMR_2(t_0).
\end{align}
So the induction step follows by \eqref{eq:thm:HardMax-biased-PMRtoPr}. Claim proved.

Now let us focus on global round $m_0$, and denote $\mN_i = \posteriorN{i}{m_0}$.  By the above claim,
\begin{align}\label{eq:thm:HardMax-biased-mN}
\text{$\mN_1$ stochastically dominates $\mN_2$, and moreover
    $\mN_i(m_0-1)>\mN_i(m_0-1)$.}
\end{align}

By definition of $m_0$, either (i) \alg[2] deviates from \DynGreedy starting from local step $m_0$, which implies $\rew_1(m_0)> \rew_2(m_0)$ by Lemma~\ref{lm:DG-rew}, or (ii) \DynGreedy deviates from \StaticGreedy starting from local step $m_0$, which implies $\rew_1(m_0)>\rew_1(m_0-1)$ by Lemma~\ref{dgmono}. In both cases, using \eqref{eq:thm:HardMax-biased-rew} and \eqref{eq:thm:HardMax-biased-mN}, it follows that the inequality in \eqref{eq:thm:HardMax-biased-PMR-aux} is strict for $t_0=m_0$.

Therefore, agent $m_0$ chooses principal $1$, and by Lemma~\ref{lm:DG-sudden} so do all subsequent agents.



%%%%%%%%%%%%
\subsection{The main result for \HardMaxRandom:
Proof of Theorem~\ref{thm:random-clean}}
\label{sec:proofs-HMR-main}

Without loss of generality, assume $m_0 = n_0$.
Consider global round $t\geq n_0$. Recall that each agent chooses principal $1$ with probability at least
    $\respF(-1)>0$.

Then
    $\E[n_1(t+1)] \geq 2\eps_0\,t $.
By Chernoff Bounds (Theorem~\ref{thm:chernoff}), we have that
    $n_1(t+1)\geq \eps_0 t$
holds with probability at least $1-q$,
where $q = \exp(-\eps_0 t/12)$.

We need to prove that
    $\PMR_1(t) - \PMR_2(t)>0$.
For any $m_1$ and $m_2$, consider the quantity
\[ \Delta(m_1,m_2) := \BIR_2(m_2+1) - \BIR_1(m_1+1).\]
Whenever $m_1 \geq \eps_0 t/2 -1$ and $m_2<t$, it holds that
\begin{align*}
\Delta(m_1,m_2) \geq \Delta(\eps_0 t / 2, t)
    \geq \BIR_2(t)/2.
\end{align*}
The above inequalities follow, resp., from algorithms' \bmonotonicity and \eqref{eq:random-better-weaker}. Now,
\begin{align*}
\PMR_1(t) - \PMR_2(t)
    &= \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}{\Delta(m_1,m_2)} \\
    &\geq -q
        + \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}
            {\Delta(m_1,m_2)\mid m_1 \geq \eps_0 t/2-1} \\
    &\geq \BIR_2(t)/2-q \\
    &> \BIR_2(t)/4 > 0
    &\EqComment{by \refeq{eq:random-assn}}. \qedhere
\end{align*}

%%%%%%%%%%%%
\subsection{A little greedy goes a long way
(Proof of Theorem~\ref{thm:random-greedy})}
\label{sec:proofs-HMR-main}


%\begin{proof}[Proof of Theorem~\ref{thm:random-greedy}]
  Let $\rewgr(n)$ denote the Bayesian-expected reward of the ``greedy
  choice'' after after $n-1$ steps of \alg[1]. Note that
  $\rew_1(\cdot)$ and $\rewgr(\cdot)$ are non-decreasing: the former
  because \alg[1] is \bmonotone and the latter because the ``greedy
  choice'' is only improved with an increasing set of
  observations, see Lemma~\ref{lm:MII}.
Using \eqref{eq:BIR-modification}, we conclude that
the greedy modification \alg[2] is \bmonotone.

  By definition of the ``greedy choice,'' $\rew_1(n)\leq \rewgr(n)$
  for all steps $n$. Moreover, by Lemma~\ref{lm:DG-rew},
  \alg[1] has a strictly smaller $\rew(n_0)$ compared to \DynGreedy;
  so, $\rew_1(n_0)<\rew_2(n_0)$.

Let $\alg$ denote a copy of \alg[1] that is running ``inside" \alg[2]. Let $m_2(t)$ be the number of global rounds before $t$ in which the agent chooses principal $2$ \emph{and} \alg is invoked; \ie it is the number of agents seen by \alg before global round $t$. Let $\mM_{2,t}$ be the agents' posterior distribution for $m_2(t)$.

We claim that in each global round $t\geq n_0$, distribution $\mM_{2,t}$ stochastically dominates distribution $\posteriorN{1}{t}$, and $\PMR_1(t)<\PMR_2(t)$. We use induction on $t$. The base case $t=n_0$ holds because $\mM_{2,t} = \posteriorN{1}{t}$ (because the two algorithms coincide on the first $n_0-1$ steps), and $\PMR_1(n_0)<\PMR_2(n_0)$ is proved as in \eqref{eq:pf:thm:DG-dominance}, using the fact that $\rew_1(n_0)<\rew_2(n_0)$.

The induction step is proved as follows. The induction hypothesis for global round $t-1$ implies that agent $t-1$ is seen by \alg with probability $(1-\eps_0)(1-p)$, which is strictly larger than $\eps_0$, the probability with which this agent is seen by \alg[2]. Therefore, $\mM_{2,t}$ stochastically dominates $\posteriorN{1}{t}$.
\begin{align}
\PMR_1(t)
  &= \Ex{n\sim \posteriorN{1}{t}} {\rew_1(n+1)} \nonumber \\
  &\leq \Ex{m\sim \mM_{2,t}} {\rew_1(m+1)}
    \label{eq:pf:thm:random-greedy-1}\\
  &< \Ex{m\sim \mM_{2,t}} {(1-p)\cdot \rew_1(m+1) + p\cdot \rewgr(m+1)}
    \label{eq:pf:thm:random-greedy-2} \\
  &= \PMR_2(t). \nonumber
\end{align}
Here \eqref{eq:pf:thm:random-greedy-1} holds because $\rew_1(\cdot)$ is \bmonotone and $\mM_{2,t}$ stochastically dominates $\posteriorN{1}{t}$, and inequality \eqref{eq:pf:thm:random-greedy-2} holds because $\rew_1(n_0)<\rew_2(n_0)$ and $\mM_{2,t}(n_0)>0$.%
\footnote{If $\rew_1(\cdot)$ is strictly increasing, then  \eqref{eq:pf:thm:random-greedy-1} is strict, too; this is because $\mM_{2,t}(t-1)>\posteriorN{1}{t}(t-1)$.  }
%\end{proof}



%%%%%%%%%%%%
\subsection{\SoftMaxRandom: proof of Theorem~\ref{thm:SoftMax-strong}}
\label{sec:proofs-SoftMax}

%\begin{proof}[Proof of Theorem~\ref{thm:SoftMax-strong}]
  Let $\beta_1 = \min\{c_0'\delta_0, \beta_0/20\}$ with $\delta_0$
  defined in~\eqref{eq:SoftMax-smooth}.  Recall each agent chooses
  \alg[1] with probability at least $\respF(-1)= \eps_0$.  By
By condition \eqref{eq:SoftMax-assn-strong} and the fact that
$\BIR_1(n) \to 0$,
  there exists some sufficiently large $T_1$ such that for any
  $t\geq T_1$, $\BIR_1(\eps_0 T_1/2) \leq \beta_1/c_0'$ and
  $\BIR_2(t) > e^{-\eps_0 t/12}$. Moreover, for any $t\geq T_1$, we
  know $\E[n_1(t+1)] \geq \eps_0\,t $, and by the Chernoff Bounds
  (Theorem~\ref{thm:chernoff}), we have $n_1(t+1) \geq \eps_0 t/2$
  holds with probability at least $1 - q_1(t)$ with
  $q_1(t) = \exp(-\eps_0 t/12) < \BIR_2(t)$. It follows that for any $t\geq T_1$,
\begin{align*}
  \PMR_2(t) - \PMR_1(t) &= \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}{\BIR_1(m_1+ 1) - \BIR_2(m_2+1)} \\
                        &\leq q_1(t)  + \Ex{m_1\sim \posteriorN{1}{t}}{\BIR_1(m_1+ 1)\mid m_1 \geq \eps_0 t/2 - 1 } - \BIR_2(t)\\
                        &\leq \BIR_1(\eps_0 T_1/2) \leq \beta_1/c_0'
\end{align*}
% where the second inequality follows from~\eqref{eq:random-assn}.
Since the response function $\respF$ is $c_0'$-Lipschitz in the
neighborhood of $[-\delta_0, \delta_0]$, each agent after round $T_1$
will choose \alg[1] with probability at least
\[
  p_t \geq \tfrac{1}{2} - c_0'\left(\PMR_2(t) - \PMR_1(t)\right) \geq
  \tfrac{1}{2} - \beta_1.
\]

Next, we will show that there exists a sufficiently large $T_2$ such
that for any $t\geq T_1 + T_2$, with high probability
$n_1(t) > \max\{n_0, (1 - \beta_0)n_2(t)\}$, where $n_0$ is defined
in~\eqref{eq:SoftMax-better}. %  Let us first lower bound the number of
% agents received by \alg[1] after some number of rounds $t = T_1 + T'$
% for any $T' \geq T_1$.
Fix any $t \geq T_1 + T_2$.
Since each agent chooses \alg[1] with probability at least
$1/2 - \beta_1$, by Chernoff Bounds (Theorem~\ref{thm:chernoff}) we
have with probability at least $1 - q_2(t)$ that the number of agents
that choose \alg[1] is at least
    $\beta_0(\nicefrac{1}{2} - \beta_1)t/5$,
where %the function
$$
q_2(x) = \exp\rbr{ -\nicefrac{1}{3}\;
    (\nicefrac{1}{2} - \beta_1)(1 - \beta_0/5)^2x} .
$$
The number of agents received by \alg[2] is at most
$T_1 + (\nicefrac{1}{2} + \beta_1)t + (\nicefrac{1}{2} - \beta_1)(1 - \beta_0/5)t$.

Then as long as
% $T_2 \geq \max\left\{\frac{3T_1}{(1 - \beta_0)}, 8 n_0\right\}$, we
% can guarantee that for any $t \geq T_1 + T_2$,
% $n_1(t) > n_2(t) (1 - \beta_0)$ and $n_1(t) > n_0$ with probability at
% least $1 - q_2(t)$.
$T_2 \geq \frac{5T_1}{\beta_0}$, we can guarantee that
$n_1(t) > n_2(t) (1 - \beta_0)$ and $n_1(t) > n_0$ with probability at
least $1 - q_2(t)$ for any $t \geq T_1 + T_2$.
% for any $t\geq T_1 + T_2$, as long as $T_2 \geq .$ Moreover, to
% guarantee that , we will just need $T_2 \geq$.  Finally, we will
% argue that in each round $t \geq T_1 + T_2$, we recover the
% guarantee in \eqref{eq:thm:SoftMax-strong}.
% % \[
% %   \Pr[i_t = 1] \geq \frac{1}{2} + \frac{c_0\alpha_0\BIR_2(t)}{4}.
% % \]
Note that the weak BIR-dominance condition
in~\eqref{eq:SoftMax-better} implies that for any $t\geq T_1 + T_2$
with probability at least $1 - q_2(t)$, we have
$ \BIR_1(n_1(t)) < (1- \alpha_0)\,\BIR_2(n_2(t))$.

It follows that for any $t\geq T_1 + T_2$,
\begin{align*}
  \PMR_1(t) - \PMR_2(t) &= \Ex{m_1\sim \posteriorN{1}{t},\;m_2\sim \posteriorN{2}{t}}{\BIR_2(m_2+ 1) - \BIR_1(m_1+1)} \\
                        &\geq (1 - q_2(t))\,\alpha_0\, \BIR_2(t) - q_2(t)
                        \geq \alpha_0\, \BIR_2(t)/4,
\end{align*}
where the last inequality holds as long as
$q_2(t) \leq \alpha_0\BIR_2(t)/4$, and is implied by the condition
in~\eqref{eq:SoftMax-assn-strong} as long as $T_2$ is sufficiently
large. Hence, by the definition of our \SoftMaxRandom response
function and assumption in~\eqref{eq:SoftMax-smooth}, we have
$  \Pr[i_t = 1] \geq \nicefrac{1}{2} +
    \nicefrac{1}{4}\; c_0\,\alpha_0\,\BIR_2(t)$.
%\end{proof}

