\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{amsfonts}
\usepackage[margin=0.5in]{geometry}
          
             

\begin{document} 

\title{Competing Bandits}
\author{Guy Aridor, Kevin Liu}
\maketitle

\section*{Introduction / Setup}

In this project we look at a game played between two competing bandit algorithms:
\begin{enumerate}
\item Agents (who can be thought of as customers or users) choose between two competing principals (who can be thought of as firms). Each agent does so based on running an algorithm and some beliefs about the rewards they will receive from each principal. Agents seek to maximize their reward. In our implementation, an agent's beliefs are based on the realized rewards that prior agents received, which differs from the paper (in which each agent has no information on what happened before them). There are $T$ agents in each simulation, so the simulation lasts for $T$ rounds.
\item Before the beginning of the game, each principal selects a bandit algorithm to use, and starts with some prior beliefs on the distributions of the $K$ arms. Each time the principal is chosen by an agent, the principal runs its bandit algorithm and selects an arm. The reward (drawn from a Bernoulli distribution in our simulations) is given to the agent, and the $(arm, reward)$ data point is used to Bayesian update the principal's posterior (which is a Beta distribution) on that arm. Each principal seeks to maximize its market share (the fraction of agents that chooses them), and can do so by choosing arms with high rewards so that agents will choose them. 
\item At each $t = 1,...,T$, a single agent enters (and lives only for one period) and selects a principal according to the Agent algorithm (throughout the simulation, every agent uses the same algorithm) and the agent's beliefs (which, in our implementation, varies between agents according to past information).
\item The chosen principal runs its bandit algorithm to select an arm. The reward generated by the arm is given to the agent. The principal not chosen is not aware of the $(arm, reward)$ pair.
\end{enumerate}

This is based off the setup of the model in Mansour, Slivkins, and Wu (2017). For more specific details on the model, please refer to their paper. We note any meaningful departures we took from their paper in this paper. We wrote a variety of simulations to look to see if we could reproduce the original results in the paper as well as a few experiments testing certain pertubations of the model.

The first step was to take the model in the paper and make it amenable to simulation as the methodology in the theoretical analysis of the paper made it hard to simulate the beliefs of the agent. To tackle this we assumed that the agent had access to a ``reputation" score that came from a sliding window average of the past $n$ agents that had picked this principal.

Then, we implemented the following four behavioral algorithms that describe the decision rule of the agent given the reputation score:
\begin{enumerate}
\item HardMax: The agent chooses the firm with a larger score, breaking ties uniformly
\item HardMaxRandom: Each agent uses HardMax with probability $1 - \epsilon$, and chooses between the principals uniformly at random with probability $\epsilon$
\item SoftMax: The principals are chosen randomly according to a logistic function, whose steepness is controlled by $\alpha$, and each principal has a minimum probability $\epsilon$ of being chosen. Further detail is given in a section below. 
\item Uniform: Each agent chooses between the two principals uniformly at random. Though we implemented this algorithm, we did not end up using it. 
\end{enumerate}

In the simulations we maintained the assumption in the paper that the principals commit to a learning algorithm in the first period and thus we implemented standard bandit learning algorithms for the principals. In general, we were interested in thinking about the differences that would arise when the principals were running greedy algorithms, non-adaptive exploration algorithms, or adaptive exploration algorithms. Bandit algorithms that we implemented include:
\begin{enumerate}
\item ExploreThenExploit: The algorithm takes $X$ exploration steps divided evenly among the $K$ arms, according to a modulus function. After the $X$ exploration steps, it always selects the arm it believes has the highest mean.
\item StaticGreedy: Always select the arm whose mean is highest according to the prior distributions.
\item DynamicGreedy: Each round it's run, it selects the arm whose mean is highest according to the posterior distributions at that moment in time.
\item DynamicEpsilonGreedy: With probability $\epsilon$, picks an arm at random. With probability $1-\epsilon$, runs DyanmicGreedy according to the above. 
\item ThompsonSampling: For each of the $K$ posterior distributions, sample a point from each distribution. Take the highest point, and choose the arm corresponding to the distribution that the point came from.
\item UCB: Select the arm whose confidence interval has the highest upper bound. 
\end{enumerate}

\section*{Summary of Results}
In this project, we looked at a competing bandits framework and explored several experiments using it. In our first experiment we looked at the effects on exploration from imposing finite memory on the agents so that they quickly forget the past. In this experiment we showed that with sufficiently low memory we can generate exploration even when agents are perfect expected utility maximizers. In our second experiment we ran an exhaustive experiment comparing the results of various learning algorithms competing against each other. [Talk about results]. In our third experiment we explored the consequences of different parameterizations of the SoftMax behavioral model. [Talk about results]. In our fourth experiment we tried to answer the question of if the learning algorithm or correctness of the initial information is more important in determining which principal gets higher market share. Our simulations indicated that with enough time, adaptive exploration with poor initial information will catch up with a static greedy algorithm with better initial information. Non-adaptive exploration however will catch up at a slower pace. Our fifth experiment attempted to explore what the consequences of giving the principals free information was if the agents were expected utility maximizers and we found that this has virtually no effect.

\section*{Experiment 1 - Finite Memory}
In our simulations we encoded agents' beliefs about the principals as coming from a sliding window average of the rewards previous agents had experienced. One set of simulations that we ran considered the consequences of changing the size of this window. This can be interpreted as agents having finite memory so that ``bad" actions by the principals in the past will be forgotten at some point. Intuitively one would expect this to increase exploration even if we fixed agents as playing $HardMax$ and thus being expected utility maximizers. In the paper the main result around $HardMax$ was that ANY deviation from dynamic greedy would cause a principal to lose all remaining agents. However, if there is finite memory we don't necessarily expect this to be the case.

There are two reasonable ways to define finite memory in the context of this model, but we report results from only one. The results we report are both the market share that the principals get as well as the regret that they incur since we want to determine both if there is more exploration and if there are differences in the resulting share of agents they get (i.e. if they can recover from ``bad"' choices).

The two definitions of finite memory are:
\begin{enumerate}
\item Agents remember only the last $n$ periods, no matter who was picked. If a principal wasn't picked in the last $n$ periods, then we default back to the original prior.
\item Agents form their score for a principal based on the last $n$ observations they have from that principal.
\end{enumerate}

We only simulate definition 2 and the results are as follows:

\begin{center}
Agent: HardMax \\Principals: Thompson Sampling vs Thompson Sampling
 \begin{tabular}{||c c c c c||} 
 \hline
 Memory Size & Principal 1 MS & Principal 2 MS & Principal 1 Avg Regret & Principal 2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 1 & 0.498 & 0.502 & 0.021 & 0.021 \\ 
 \hline
 5 & 0.566 & 0.433 & 0.041 & 0.068 \\
 \hline
 10 & 0.540 & 0.460 & 0.062 & 0.080 \\
 \hline
 50 & 0.480 & 0.520 & 0.109 & 0.103 \\
 \hline
 100 & 0.320 & 0.680 & 0.130 & 0.067 \\ [1ex] 
 \hline
\end{tabular}

\vspace{0.75cm}
Agent: HardMax \\Principals: Thompson Sampling vs Dynamic $\epsilon$-Greedy
 \begin{tabular}{||c c c c c||} 
 \hline
 Memory Size & Dynamic $\epsilon$-Greedy MS  & Thompson Sampling MS & D $\epsilon$ Avg Regret & TS Avg Regret  \\ [0.5ex] 
 \hline\hline
 1 & 0.495 & 0.505 & 0.035 & 0.032 \\ 
 \hline
 5 & 0.607 & 0.3926 & 0.052 & 0.079 \\
 \hline
 10 & 0.531 & 0.469 & 0.061 & 0.065 \\
 \hline
 50 & 0.402 & 0.598 & 0.131 & 0.0801 \\
 \hline
 100 & 0.520 & 0.480 & 0.111 & 0.109 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

These results show that regret roughly increases as the size of memory increases. One of the results from the paper our setting is based off is that in HardMax the principals cannot do any exploration otherwise they'll lose for the remaining periods. In this simulation, when memory is large, we see that in a particular simulation (as opposed to on aggregate) one of the principals always ends up getting almost every agent that comes into the market and there is little exploration, which is in line with the results from the paper. However, when memory is small, there is a more even split of the market in a given simulation and we have that average regret is lower (and thus there is more exploration on average in a given simulation). This makes sense given the fact that agents have a short-term memory and thus principals may not necessarily be punished for too long for sub-optimal decisions. Thus we see that we can get ``innovation" even without any behavioral deviations by assuming that agents forget the past eventually.

\section*{Experiment 2 - Which Algorithm Wins?}
This experiment mainly looks at which learning algorithm ``wins" when the two firms do not play the same algorithm. In this experiment we fix the priors of the principals to be identical over the arms and the agents to be identical over the principals at the beginning of the simulation. 

We looked at each behavioral assumption and asked, fixing that behavioral assumption, is there any case where playing a better learning algorithm helps the principal get a higher market share?

\begin{center}
Agent: HardMax, T = 5000
 \begin{tabular}{||c c c c c c||} 
 \hline
 Principal 1 Alg & Principal 2 Alg & Principal 1 MS & Principal 2 MS & P1 Avg Regret & P2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 StaticGreedy & ExploreThenExploit & 0.352 & 0.648 & 0.200 & 0.084 \\ 
 \hline
 StaticGreedy & Dynamic $\epsilon$-greedy & 0.510 & 0.490 & 0.2 & 0.098\\
 \hline
 DynamicGreedy & ExploreThenExploit & 0.444 & 0.556 & 0.129 & 0.093 \\ 
 \hline
 DynamicGreedy & Dynamic $\epsilon$-greedy & 0.486 & 0.514 & 0.124 & 0.111 \\
 \hline
  UCB1 & ExploreThenExploit & 0.360 & 0.640 & 0.156 & 0.080 \\
  \hline
  UCB1 & Dynamic $\epsilon$-greedy & 0.335 & 0.664 & 0.138 & 0.100 \\
  \hline
  ThompsonSampling & ExploreThenExploit & 0.480 & 0.512 & 0.094 & 0.102 \\
  \hline
  ThompsonSampling & Dynamic $\epsilon$-greedy & 0.559 & 0.441 & 0.086 & 0.118 \\
  \hline
    ThompsonSampling & StaticGreedy & 0.335 & 0.664 & 0.138 & 0.100 \\
  \hline
    ThompsonSampling & DynamicGreedy & 0.508 & 0.492 & 0.085 & 0.137 \\
  \hline
  UCB1 & StaticGreedy & 0.640& 0.360 & 0.119 & 0.2 \\
  \hline
      UCB1 & DynamicGreedy & 0.713 & 0.287 & 0.111 & 0.180\\
  \hline
   UCB1 & ThompsonSampling & 0.40 & 0.600 & 0.148 & 0.086 \\
  \hline
\end{tabular}
\end{center}

\section*{Experiment 3 - Tuning SoftMax} % might want to put this one at the top
This experiment mainly looked at what happens if we do a grid search over alpha parameters in tuning SoftMax. % you have to tune both alpha and epsilon for SoftMax

[insert plots]
\section*{Experiment 4 - Prior or Algorithm}
A natural question to ask in this context is whether the correctness of the initial information that the principals have matters more than the learning algorithm they employ. Specifically, we want to test what happens if we fix one principal as having a better prior than the other but that principal plays $StaticGreedy$ and thus only uses their original priors to decide on their actions.   Suppose the other principal has a more ``incorrect" set of initial beliefs but employs a more sophisticated, adaptive exploration algorithm such as Thompson Sampling. Will the principal playing a smarter learning algorithm catch up eventually (for any of the behavioral assumptions) or is the original prior more important?

We simulate this in the following scenario. We set $K = 10$ where 8 of the arms have approximately identical means ($Bernoulli(0.45)$), one of the arms (denote it as arm $B$) has a higher mean ($Bernoulli(0.55)$), and we vary the mean of the ``best" arm (denote it as arm $A$) in increments of 0.1. Thus, we consider a simulation where arm $A$ has true distributions of $Bernoulli(0.55), Bernoulli(0.65), Bernoulli(0.75)$.

We suppose that the ``dumb" principal 2 has relatively correct beliefs in that she has a prior such that arm $B$ is the best arm. We suppose that the ``smart" principal 1 has perverse beliefs such that she has a prior that the 8 worst arms have the best mean reward ($Beta(0.45, 0.55)$), arm $B$ has slightly worse mean reward ($Beta(0.4, 0.6$), and arm $A$ has substantially worse mean reward ($Beta(0.1, 0.9)$).

We run simulations under this setup to see if the ``smart" principal's learning algorithm can let her overcome the bad beliefs.

\begin{center}
Agent: HardMax, T = 5000
 \begin{tabular}{||c c c c c c||} 
 \hline
 Mean of Arm A & Principal 1 Alg & Principal 1 MS & Principal 2 MS & P1 Avg Regret & P2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 0.55 & ThompsonSampling & 0.440 & 0.560 & 0.049 & 0.000 \\ 
 \hline
 0.65 & ThompsonSampling & 0.647 & 0.353 &  0.057 &  0.100 \\
 \hline
 0.75 & ThompsonSampling & 0.710 & 0.291 & 0.059  & 0.200 \\
 \hline
 0.55 & Dynamic $\epsilon$-greedy & 0.298 & 0.656 & 0.057 & 0.000 \\
 \hline
  0.65 & Dynamic $\epsilon$-greedy & 0.335 & 0.664 & 0.138 & 0.100 \\
 \hline
  0.75 & Dynamic $\epsilon$-greedy & 0.442 & 0.558 & 0.154 & 0.200 \\[1ex]
  \hline
\end{tabular}
\end{center}
\vspace{0.75cm}
\begin{center}
Agent: HardMaxWithRandom, T = 5000
 \begin{tabular}{||c c c c c c||} 
 \hline
 Mean of Arm A & Principal 1 Alg & Principal 1 MS & Principal 2 MS & P1 Avg Regret & P2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 0.55 & ThompsonSampling & 0.317 & 0.683 & 0.031 & 0.000 \\ 
 \hline
 0.65 & ThompsonSampling & 0.630 & 0.369 &  0.028 &  0.100 \\
 \hline
 0.75 & ThompsonSampling & 0.787 & 0.291 & 0.015  & 0.200 \\
 \hline
 0.55 & Dynamic $\epsilon$-greedy & 0.343 & 0.702 & 0.038 & 0.000 \\
 \hline
  0.65 & Dynamic $\epsilon$-greedy & 0.400 & 0.600 & 0.076 & 0.100 \\
 \hline
  0.75 & Dynamic $\epsilon$-greedy &0.615 & 0.385 & 0.074 & 0.200 \\[1ex]
  \hline
\end{tabular}
\end{center}

\vspace{0.75cm}
\begin{center}
Agent: SoftMax, T = 5000
 \begin{tabular}{||c c c c c c||} 
 \hline
 Mean of Arm A & Principal 1 Alg & Principal 1 MS & Principal 2 MS & P1 Avg Regret & P2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 0.55 & ThompsonSampling & 0.442 & 0.558 & 0.027 & 0.000 \\ 
 \hline
 0.65 & ThompsonSampling & 0.600 & 0.400 &  0.025 &  0.100 \\
 \hline
 0.75 & ThompsonSampling & 0.721 & 0.280 & 0.026  & 0.200 \\
 \hline
 0.55 & Dynamic $\epsilon$-greedy & 0.442 & 0.556 & 0.038 & 0.000 \\
 \hline
  0.65 & Dynamic $\epsilon$-greedy & 0.400 & 0.600 & 0.021 & 0.100 \\
 \hline
  0.75 & Dynamic $\epsilon$-greedy & 0.666 & 0.334 & 0.050 & 0.200 \\[1ex]
  \hline
\end{tabular}
\end{center}
\vspace{0.25cm}
Regardless of the behavioral model of the agent, we see that the ``dumb" principal 2 wins a higher share of the market than principal 1 when she happens to have correct prior beliefs about the best arm (when the distribution of arm $A$ is $Bernoulli(0.55)$). This happens whether principal 1 plays an adaptive or a non-adaptive exploration algorithm. Intuitively this makes sense as the ``dumb" principal will end up getting regret 0 simply because of her correct beliefs and it is incredibly hard for a smarter learning algorithm to make up for this. \\
\\
However, as we increase the reward of arm $A$ such that principal 2 no longer has completely correct beliefs (she believes the second-best arm is the best arm), we see that when principal 1 plays an adaptive exploration algorithm she catches up very quickly and takes most of the market. However, when principal 1 plays a non-adaptive exploration algorithm we see that the ``dumb" algorithm with better initial information still wins out, especially when the agent is HardMax. As we make the agent ``more behavioral", the non-adaptive exploration algorithm becomes progressively better.\\
\\
The previous results show that, especially when playing an adaptive exploration algorithm, it is possible that a better learning algorithm helps overcome initial bad information even if the opponent has almost correct information. However, one expects this takes time and that perhaps if we set $T$ to be lower, the ``dumb" principal would still win. To test this, we re-run the same simulations as above but set $T=500$

\begin{center}
Agent: HardMax, T = 500
 \begin{tabular}{||c c c c c c||} 
 \hline
 Mean of Arm A & Principal 1 Alg & Principal 1 MS & Principal 2 MS & P1 Avg Regret & P2 Avg Regret  \\ [0.5ex] 
 \hline\hline
 0.55 & ThompsonSampling & 0.181 & 0.812 & 0.088 & 0.0 \\
 \hline
 0.65 & ThompsonSampling & 0.287 & 0.712 & 0.178 &  0.100 \\
 \hline
 0.75 & ThompsonSampling & 0.274 & 0.723 & 0.244  & 0.200 \\
 \hline
 0.55 & Dynamic $\epsilon$-greedy & 0.183 & 0.817 & 0.100 & 0.000 \\
 \hline
  0.65 & Dynamic $\epsilon$-greedy & 0.169 & 0.831 & 0.190 & 0.100 \\
 \hline
  0.75 & Dynamic $\epsilon$-greedy & 0.297 & 0.703 & 0.251 & 0.200 \\[1ex]
  \hline
\end{tabular}
\end{center}
\vspace{0.25cm}
This demonstrates that part of the results from above were driven by the large $T$ and that with a low $T$ of 500, the ``smart" learning algorithm does not have enough time to overcome the bad initial information. Thus we conclude that the ``smart" learning algorithm will matter if the number of rounds is sufficiently high but for a low number of rounds, better initial information may be more important than a better learning algorithm.

\section*{Experiment 5 - Warm Start and HardMax}
What is the effect of a ``warm start" on the performance of HardMax? Our simulations show that with HardMax the initial rounds matter a lot in a particular simulation and whoever wins in the first few rounds takes the entire market. We explore if having a warm start makes it so that the market is more evenly split in a given simulation.\\

We experimented with giving a warm start of 0, 5, 25, 50, and 100 observations to each principal. We ran the following competing algorithms: UCB vs UCB, UCB vs DynamicGreedy, StaticGreedy vs UCB and observed little effect. Regardless of the algorithm that was played or the number of free observations we gave to the principals, the sequence of simulations lead to market shares in each simulation looking roughly the same. Namely, in a given simulation one of the principals would take the entire market but since the principals had the same priors over the arms and the agents had the same priors over the principals, it was random who would take the entire market. Thus, it seems that free observations for the principal do not impact the resulting market share. 

\section*{References}
\begin{enumerate}
\item Mansour Y., Slivkins A., Wu S. (2017) Competing bandits: Learning under Competition, The 9th Innovations in Theoretical Computer Science (ITCS'18)
\end{enumerate}


\end{document}