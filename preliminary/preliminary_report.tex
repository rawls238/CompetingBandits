\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{amsfonts}      
             
\graphicspath{{../results}}
             
\begin{document} 

\title{Preliminary Experiment}
\maketitle

Considered Bandit Prior/Instances that were considered (only for 10 arms). :
\begin{enumerate}
\item Uniform: Draw the mean rewards for the arms from [0.25, 0.75]
\item ``Heavy-tailed": For ease we simply took the 10 arms to be randomly drawn from Beta($\alpha=0.6,\beta=0.6$)
\item Needle-in-haystack
\begin{enumerate}
\item Low Mean
\begin{enumerate}
\item Low - 9 arms with mean 0.01, 1 arm with mean 0.02 (+ 0.01)
\item Medium - 9 arms with mean 0.01, 1 arm with mean 0.06 (+ 0.05)
\item High - 9 arms with mean 0.01, 1 arm with mean 0.21 (+ 0.20)
\end{enumerate}
\item Medium Mean
\begin{enumerate}
\item Low - 9 arms with mean 0.50, 1 arm with mean 0.01 (+ 0.01)
\item Medium - 9 arms with mean 0.50, 1 arm with mean 0.55 (+ 0.05)
\item High - 9 arms with mean 0.50, 1 arm with mean 0.70 (+ 0.20)
\end{enumerate}
\item High Mean
\begin{enumerate}
\item Low - 9 arms with mean 0.80, 1 arm with mean 0.81 (+ 0.01)
\item Medium - 9 arms with mean 0.80, 1 arm with mean 0.85 (+ 0.05)
\item High - 9 arms with mean 0.80, 1 arm with mean 1.0 (+ 0.20)
\end{enumerate}
\end{enumerate}
\end{enumerate}

Algorithms considered:
\begin{enumerate}
\item ThompsonSampling with priors of $Beta(5, 5)$ for every arm.
\item ExploreThenExploit with 100 exploration steps
\item DynamicGreedy with priors of $Beta(5, 5)$ for every arm
\item Dynamic $\epsilon$-greedy with $\epsilon$=0.05
\item UCB1
\end{enumerate}

The first run considered $N=50$ runs which didn't seem to be able to differentiate between the learning rates of the particular algorithms. However, it was helpful for reducing the space of needle in haystack algorithms. Originally we considered three regimes of ``needle-in-haystack" instances since we weren't sure if learning would be different between the different regimes. We considered so many needle-in-haystack instances to see if there would be meaningful qualitative differences between learning when the means were at extremes vs when most means were in the middle of the range of possible means. \\

The plots for all the algorithms are reported below. The ``low" $\Delta$ in the needle-in-haystack appears uninteresting across all considered means, likely becuase the $\Delta$ of $0.01$ is too small to be meaningful. For the ``medium" instances it appears as though there is a qualitative difference between the ``low" mean and the others. Though this may be partially due to low $N$, it appears as though there is some divergence particularly between UCB and Thompson Sampling. For the high $\Delta$ it the ``low" mean and ``medium" mean seem to have qualitatively similar results. The ``high" mean has starker differences in terms of the learning rates of the algorithms.

First, looking at the medium range: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle01 - Medium 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - Medium 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle80 - Medium 1"}
\\
The low range: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle01 - Low 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - Low 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle80 - Low 1"}
\\

The high range: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle01 - High 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - High 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle80 - High 1"}
\\

Surprisingly with $N=50$ there appeared to be considerable variability in the trajectories and so the experiment was reran with $N=250$ and $T=5000$. To reduce the amount of time this took, only Uniform, HeavyTail, Needle50 - Medium, and Needle50 - High were considered. Though there were some qualitative differences between the different needle in haystack instances discussed above, they were omitted here but we can run this again with those if we decide we want some more fine-grain information.

The plots with all the algorithms are reported below:
The high range: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - Medium 3"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - High 1"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Heavy Tail 3"}
\\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Uniform 3"}
\\

A few particular striking comparative plots: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - High UCB"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Heavy Tail UCB"} \\

How does UCB compare against Dynamic $\epsilon$-greedy in these cases? \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - High UCB, Dyn eps"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Heavy Tail UCB, Dyn eps"} \\

The rate of learning for UCB seems slower than for the other algorithms. Is this expected (or do we need to tune the UCB1 constant)? It appears that UCB does not catch up to Dynamic $\epsilon$-greedy until $t = 1500$. This potentially explains the results we saw that UCB1 loses in the competing bandits game because it's rate of learning growth for these bandit instances is slower than the other algorithms.

Comparing ThompsonSampling to Dynamic $\epsilon$-greedy: \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Needle50 - High TS, Dyn eps"} \\
\includegraphics[scale=0.5]{"../results/Reward Trajectory for Heavy Tail TS, Dyn eps"} \\


\end{document}