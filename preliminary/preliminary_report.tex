\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{algorithm,algpseudocode}
             
\graphicspath{{../results}}
             
\begin{document} 

\title{Preliminary Experiment}
\maketitle

\section*{Simulation Details}

$N = 200$ simulations were run. \\
\textbf{The Bandit priors that were considered}:
\begin{itemize}
\item Needle-in-haystack
\begin{enumerate}
\item High (with three arms) - 2 arms with mean 0.50, 1 arm with mean 0.7 (+ 0.2)
\item High (with twenty arms) - 19 arms with mean 0.50, 1 arm with mean 0.70 (+ 0.20)
\item Medium-High (with twenty arms) - 18 arms with mean 0.50, 1 arm with 0.60, 1 arm 0.80
\end{enumerate}
\end{itemize}
\textbf{Algorithms considered}:
\begin{enumerate}
\item ThompsonSampling with priors of $Beta(1, 1)$ for every arm.
\item DynamicGreedy with priors of $Beta(1, 1)$ for every arm
\item Bayesian Dynamic $\epsilon$-greedy with priors of $Beta(1, 1)$ for every arm and $\epsilon = 0.05$
\item Non-Bayesian $\epsilon$-greedy - the greedy decision was made based on empirical mean. When there were zero observations, assumed that the empirical mean was 0 (this seems questionable), $\epsilon = 0.05$
\item UCB1 (with constant $1$)
\end{enumerate}
\pagebreak
\textbf{Simulation Procedure}
\begin{algorithm}
\begin{algorithmic}[1]
\For{Each prior $p$}
	\For{Each experiment $i$}
		\State Use $p$ as the true distribution of the arms
		\State Generate realizations for each arm and round $t$ and instantiate bandit instance
		\For{Each algorithm $alg$}
			\State Run simulation for $T$ periods
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\section*{Results}

The confidence bands come from the standard error of the loess regression used to generate the curves. The actual datapoints, when plotted, are incredibly volatile but this volatility decreases as we increase N. Part of this comes from the fact that what is plotted is the actual realized reward and not the mean reward. \\
\subsection*{Instantaneous Realized Reward Plots}
These plots contain the fitted lines for the instantaneous realized reward. \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous Realized Reward Trajectory for Needle In Haystack 1 High 10 arms"} \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous Realized Reward Trajectory for Needle In Haystack 1 Medium 1 High 10 arms"} \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous Realized Reward Trajectory for Needle In Haystack 1 High 3 arms"} \\

\subsection*{Instantaneous True Mean Reward Plots}
These plots contain the fitted lines for the TRUE mean reward for the selected arms. \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous True Mean Reward Trajectory for Needle In Haystack 1 High 10 arms"} \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous True Mean Reward Trajectory for Needle In Haystack 1 Medium 1 High 10 arms"} \\
\includegraphics[scale=0.5]{"../results/preliminary_figures/Instantaneous True Mean Reward Trajectory for Needle In Haystack 1 High 3 arms"} \\

Comments: It does appear that using $20$ arms instead of $3$ in this case does not appear too different. It appears that Thompson Sampling and UCB learn quickly (roughly seem to flatline at $t = 2000$) and DynamicGreedy slowly improves but does not seem to get to convergence even by $t = 10000$. However, the difference between the TS and DG trajectories is not vastly different between the 3 arm and 20 arm case. One difference to note is that it seems like one thing that does change between the two is that Dynamic $\epsilon$-greedy seems to take longer to learn (doesn't even appear to converge by $t = 10000$) in the more complex cases but seems to converge as quickly as TS and UCB in the 3 arm case.
\end{document}