\documentclass[../competing_bandits.tex]{subfiles}
\begin{document}

\section{Model and Preliminaries}\label{sec:model}

%\textbf{Overview} 
We consider a game involving two firms and $T$ customers (henceforth, \emph{agents}). The game lasts for $T$ rounds. In each round, a new agent arrives, chooses among the two firms, interacts with the chosen firm, and leaves forever. 

Each interaction between a firm and an agent proceeds as follows. There is a set $A$ of $K$ actions, henceforth \emph{arms}, same for both firms and all rounds. The firm chooses an arm, and the agent experiences a numerical reward observed by the firm. Each arm corresponds to a different version of the experience that a firm can provide for an agent, and the reward corresponds to the agent's satisfaction level. The other firm does not observe anything about this interaction, not even the fact that this interaction has happened.

From each firm's perspective, the interactions with agents follow the protocol of the multi-armed bandit problem (MAB). We focus on i.i.d. rewards: each reward is drawn independently from a distribution that depends on the chosen arm but not on the firm or the round. For concreteness, each reward is either $0$ or $1$. The mean reward of each action is the same for both firms (and each round), but initially unknown.  

Before the game starts, each firm commits to an MAB algorithm, and uses this algorithm to choose its actions. Each algorithm receives a ``warm start": additional $T_0$ agents that show up before the game starts, and interact with the firm in the same way as described above.

In some of our experiments, one firm is the ``incumbent" who enters the market before the other (``entrant"). Formally, the incumbent enjoys additional $X$ rounds of the ``warm start". We treat $X$ as an exogenous element of the model, and study the consequences for a fixed $X$. 


\subsubsection{Agents.}
Agents are myopic and non-strategic: they choose among the firms so as to maximize their expected reward, without attempting to influence the firms' learning algorithms or rewards of the future users. Agents are not well-informed: they only receive a rough signal about each firm's performance before they choose a firm, and no other information.

Concretely, each firm has a \emph{reputation score}, and each agent's choice is driven by these two numbers. We posit a version of rational behavior: each agent chooses a firm with a maximal reputation score (breaking ties uniformly). The reputation score is simply a sliding window average: an average reward of the last $M$ agents that chose this firm.

\subsubsection{MAB algorithms.} We consider three classes of algorithms, ranging from more primitive to more sophisticated: 

\begin{enumerate}
\item \emph{Greedy algorithms} that strive to take actions with maximal mean reward, based on the current information. 
    
\item \emph{Exploration-separating algorithms} that separate exploration and exploitation. The ``exploitation" choices strives to maximize mean reward in the next round, and the ``exploration" choices do not use the rewards observed so far.
\item \emph{Adaptive exploration}: algorithms that combine exploration and exploitation, and sway the exploration choices towards more promising alternatives.
\end{enumerate}

For concreteness, we fix one algorithm from each class. Our pilot experiments indicate that the results do not change substantially if other algorithms are chosen. For technical reasons, we consider Bayesian versions initialized with a ``fake" prior (\ie not based on actual knowledge). We consider: 

\begin{enumerate}
\item a greedy algorithm that chooses an arm with largest posterior mean reward. We call it "Dynamic Greedy" (because the chosen arm may change over time), \DG in short.

\item an exploration-separated algorithm that in each round, \emph{explores} with probability $\eps$: chooses an arm independently and uniformly at random, and with the remaining probability \emph{exploits} according to \DG. We call it ``dynamic epsilon-greedy", \DEG in short.
    
\item an adaptive-exploration algorithm called "Thompson Sampling" (\TS). In each round, this algorithm updates the posterior distribution for the mean reward of each arm $a$, draws an independent sample $s_a$ from this distribution, and chooses an arm with the largest $s_a$.
\end{enumerate}

For ease of comparison, all three algorithms are parameterized with the same fake prior: namely, the mean reward of each arm is drawn independently from a $\Beta(1,1)$ distribution. Recall that Beta priors with 0-1 rewards form a conjugate family, which allows for simple posterior updates.

Both \DEG and \TS are classic MAB algorithms, see \cite{Bubeck-survey12,TS-survey-FTML18} for background. It is well-known that \TS is near-optimal in terms of the cumulative rewards, and \DEG is very suboptimal, but still much better than \DG.%
\footnote{Precise formulations, namely upper/lower bound on \emph{regret} of the three algorithms, are not very relevant to this paper.} In a stylized formula:
    $ \TS \gg \DEG \gg \DG $
as stand-alone MAB algorithms.


\end{document} 